{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f32abb7",
   "metadata": {},
   "source": [
    "# Обратная задача ЭЭГ\n",
    "\n",
    "Обратная задача ЭЭГ заключается в восстановлении распределения источников (тока и заряда) внутри области (например, головы) на основе измерений электрического потенциала на границе.\n",
    "\n",
    "В данном ноутбуке мы используем нейросеть для моделирования распределения заряда и тока, а также обновляем функцию потерь и пайплайн для решения этой задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, LineSearches,\n",
    "      OptimizationOptimisers, LuxCUDA, Random, ComponentArrays\n",
    "using ModelingToolkit: Interval, infimum, supremum\n",
    "using Distributions, Plots, CUDA\n",
    "\n",
    "using Random\n",
    "using TensorBoardLogger\n",
    "using ProgressBars\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1339f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::CPUDevice) (generic function with 4 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const gpud = gpu_device()\n",
    "const cpud = cpu_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c97391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dalembert_operator (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определение прямой задачи\n",
    "@parameters x, y, z, t\n",
    "@variables φ(..), Ax(..),Ay(..),Az(..), ρ(..), jx(..), jy(..), jz(..)\n",
    "A = [Ax, Ay, Az]\n",
    "j = [jx, jy, jz]\n",
    "Dxx = Differential(x)^2\n",
    "Dyy = Differential(y)^2\n",
    "Dzz = Differential(z)^2\n",
    "\n",
    "# Определение физических постоянных\n",
    "const c = 2.99792458e10 # Скорость света в вакууме (см/с)\n",
    "const ε₀ = 1.0 # Диэлектрическая постоянная вакуума в СГС (размерность отсутствует)\n",
    "const ε = 1.0  # Диэлектрическая проницаемость (отн.)\n",
    "const μ₀ = 1.0 # Магнитная постоянная вакуума в СГС (размерность отсутствует)\n",
    "const μ = 1.0  # Магнитная проницаемость (отн.)\n",
    "\n",
    "\n",
    "# Определение оператора Лапласа как функции\n",
    "function laplacian(F, params)\n",
    "    return sum((Differential(param)^2)(F) for param in params)\n",
    "end\n",
    "\n",
    "# Определение оператора Даламбера как функции\n",
    "function dalembert_operator(F, params, ε, μ, c)\n",
    "    Δ = laplacian(F, params)\n",
    "    return Δ  - (ε * μ / c^2) * (Differential(t)^2)(F)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb18d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\begin{equation}\n",
       "\\frac{\\mathrm{d}}{\\mathrm{d}y} \\frac{\\mathrm{d}}{\\mathrm{d}y} \\varphi\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{\\mathrm{d}}{\\mathrm{d}x} \\varphi\\left( x, y, z, t \\right) - 1.1127 \\cdot 10^{-21} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\varphi\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}z} \\frac{\\mathrm{d}}{\\mathrm{d}z} \\varphi\\left( x, y, z, t \\right)\n",
       "\\end{equation}\n",
       " $$"
      ],
      "text/plain": [
       "Differential(y)(Differential(y)(φ(x, y, z, t))) + Differential(x)(Differential(x)(φ(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(φ(x, y, z, t))) + Differential(z)(Differential(z)(φ(x, y, z, t)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dalembert_operator(φ(x,y,z,t), [x, y, z], ε, μ, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ba867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\begin{align}\n",
       "\\frac{\\mathrm{d}}{\\mathrm{d}y} \\frac{\\mathrm{d}}{\\mathrm{d}y} \\varphi\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{\\mathrm{d}}{\\mathrm{d}x} \\varphi\\left( x, y, z, t \\right) - 1.1127 \\cdot 10^{-21} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\varphi\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}z} \\frac{\\mathrm{d}}{\\mathrm{d}z} \\varphi\\left( x, y, z, t \\right) &=  - 12.566 \\rho\\left( x, y, z, t \\right) \\\\\n",
       "\\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{\\mathrm{d}}{\\mathrm{d}x} \\mathtt{Ax}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}z} \\frac{\\mathrm{d}}{\\mathrm{d}z} \\mathtt{Ax}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}y} \\frac{\\mathrm{d}}{\\mathrm{d}y} \\mathtt{Ax}\\left( x, y, z, t \\right) - 1.1127 \\cdot 10^{-21} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\mathtt{Ax}\\left( x, y, z, t \\right) &=  - 4.1917 \\cdot 10^{-10} \\mathtt{jx}\\left( x, y, z, t \\right) \\\\\n",
       "\\frac{\\mathrm{d}}{\\mathrm{d}y} \\frac{\\mathrm{d}}{\\mathrm{d}y} \\mathtt{Ay}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}z} \\frac{\\mathrm{d}}{\\mathrm{d}z} \\mathtt{Ay}\\left( x, y, z, t \\right) - 1.1127 \\cdot 10^{-21} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\mathtt{Ay}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{\\mathrm{d}}{\\mathrm{d}x} \\mathtt{Ay}\\left( x, y, z, t \\right) &=  - 4.1917 \\cdot 10^{-10} \\mathtt{jy}\\left( x, y, z, t \\right) \\\\\n",
       "\\frac{\\mathrm{d}}{\\mathrm{d}z} \\frac{\\mathrm{d}}{\\mathrm{d}z} \\mathtt{Az}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}y} \\frac{\\mathrm{d}}{\\mathrm{d}y} \\mathtt{Az}\\left( x, y, z, t \\right) - 1.1127 \\cdot 10^{-21} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\mathtt{Az}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{\\mathrm{d}}{\\mathrm{d}x} \\mathtt{Az}\\left( x, y, z, t \\right) &=  - 4.1917 \\cdot 10^{-10} \\mathtt{jz}\\left( x, y, z, t \\right) \\\\\n",
       "3.3356 \\cdot 10^{-11} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\varphi\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}y} \\mathtt{Ay}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}z} \\mathtt{Az}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}x} \\mathtt{Ax}\\left( x, y, z, t \\right) &= 0\n",
       "\\end{align}\n",
       " $$"
      ],
      "text/plain": [
       "5-element Vector{Equation}:\n",
       " Differential(y)(Differential(y)(φ(x, y, z, t))) + Differential(x)(Differential(x)(φ(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(φ(x, y, z, t))) + Differential(z)(Differential(z)(φ(x, y, z, t))) ~ -12.566370614359172ρ(x, y, z, t)\n",
       " Differential(x)(Differential(x)(Ax(x, y, z, t))) + Differential(z)(Differential(z)(Ax(x, y, z, t))) + Differential(y)(Differential(y)(Ax(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(Ax(x, y, z, t))) ~ -4.191690043903363e-10jx(x, y, z, t)\n",
       " Differential(y)(Differential(y)(Ay(x, y, z, t))) + Differential(z)(Differential(z)(Ay(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(Ay(x, y, z, t))) + Differential(x)(Differential(x)(Ay(x, y, z, t))) ~ -4.191690043903363e-10jy(x, y, z, t)\n",
       " Differential(z)(Differential(z)(Az(x, y, z, t))) + Differential(y)(Differential(y)(Az(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(Az(x, y, z, t))) + Differential(x)(Differential(x)(Az(x, y, z, t))) ~ -4.191690043903363e-10jz(x, y, z, t)\n",
       " 3.33564095198152e-11Differential(t)(φ(x, y, z, t)) + Differential(y)(Ay(x, y, z, t)) + Differential(z)(Az(x, y, z, t)) + Differential(x)(Ax(x, y, z, t)) ~ 0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Уравнение\n",
    "eqs = [\n",
    "\n",
    "    [dalembert_operator(φ(x, y, z, t), [x, y, z], ε, μ, c) ~ -4 * pi * ρ(x, y, z, t) / ε];\n",
    "    [dalembert_operator(A[i](x, y, z, t), [x, y, z], ε, μ, c) ~ -μ * 4 * pi / c* j[i](x, y, z, t) for i in 1:3];\n",
    "    (Differential(x)(Ax(x, y, z, t)) + Differential(y)(Ay(x, y, z, t)) + Differential(z)(Az(x, y, z, t)) + (ε * μ / c) * Differential(t)(φ(x, y, z, t))) ~ 0.0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a6dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\begin{align}\n",
       "\\varphi\\left( -10, y, z, t \\right) &= 0 \\\\\n",
       "\\varphi\\left( 10, y, z, t \\right) &= 0 \\\\\n",
       "\\varphi\\left( x, -10, z, t \\right) &= 0 \\\\\n",
       "\\varphi\\left( x, 10, z, t \\right) &= 0 \\\\\n",
       "\\varphi\\left( x, y, -10, t \\right) &= 0 \\\\\n",
       "\\varphi\\left( x, y, 10, t \\right) &= 0 \\\\\n",
       "\\mathtt{Ax}\\left( -10, y, z, t \\right) &= 0 \\\\\n",
       "\\mathtt{Ay}\\left( -10, y, z, t \\right) &= 0 \\\\\n",
       "\\mathtt{Az}\\left( -10, y, z, t \\right) &= 0\n",
       "\\end{align}\n",
       " $$"
      ],
      "text/plain": [
       "9-element Vector{Equation}:\n",
       " φ(-10.0, y, z, t) ~ 0.0\n",
       " φ(10.0, y, z, t) ~ 0.0\n",
       " φ(x, -10.0, z, t) ~ 0.0\n",
       " φ(x, 10.0, z, t) ~ 0.0\n",
       " φ(x, y, -10.0, t) ~ 0.0\n",
       " φ(x, y, 10.0, t) ~ 0.0\n",
       " Ax(-10.0, y, z, t) ~ 0.0\n",
       " Ay(-10.0, y, z, t) ~ 0.0\n",
       " Az(-10.0, y, z, t) ~ 0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Границы области\n",
    "const x_min, x_max = -10.0, 10.0\n",
    "const y_min, y_max = -10.0, 10.0\n",
    "const z_min, z_max = -10.0, 10.0\n",
    "const t_min, t_max = 0.0, 1.0\n",
    "# Область\n",
    "domains = [x ∈ Interval(x_min, x_max),\n",
    "           y ∈ Interval(y_min, y_max),\n",
    "           z ∈ Interval(z_min, z_max), \n",
    "           t ∈ Interval(t_min, t_max)]\n",
    "# Начальные условия\n",
    "\n",
    "function analytic_sol_func(t, x, y, z)\n",
    "    r = sqrt((x)^2 + (y)^2 + (z)^2)\n",
    "    (t + 1)^2 / r\n",
    "end\n",
    "\n",
    "# Генерация случайных точек в пределах домена\n",
    "num_points = 100\n",
    "measured_points = []\n",
    "for _ in 1:num_points\n",
    "    x_p = rand(x_min/2:x_max/2)\n",
    "    y_p = rand(y_min/2:y_max/2)\n",
    "    z_p = rand(z_min/2:z_max/2)\n",
    "    t_p = rand(t_min/2:t_max/2)\n",
    "    # Добавление случайной точки в массив measured_points\n",
    "    phi_p = analytic_sol_func(t_p, x_p, y_p, z_p)\n",
    "    push!(measured_points, [x_p, y_p, z_p, t_p, phi_p])\n",
    "end\n",
    "for _ in 1:num_points\n",
    "    x_p = rand(x_min/2:x_max/2)\n",
    "    y_p = rand(y_min/2:y_max/2)\n",
    "    z_p = rand(z_min/2:z_max/2)\n",
    "    t_p = 0.0\n",
    "    # Добавление случайной точки в массив measured_points\n",
    "    phi_p = analytic_sol_func(t_p, x_p, y_p, z_p)\n",
    "    push!(measured_points, [x_p, y_p, z_p, t_p, phi_p])\n",
    "end\n",
    "measured_points = measured_points |> gpud\n",
    "bcs = [\n",
    "    [φ(x_min, y, z, t) ~ 0.0, φ(x_max, y, z, t) ~ 0.0,\n",
    "    φ(x, y_min, z, t) ~ 0.0, φ(x, y_max, z, t) ~ 0.0,\n",
    "    φ(x, y, z_min, t) ~ 0.0, φ(x, y, z_max, t) ~ 0.0];\n",
    "    [A[i](x_min, y, z, t)  ~ 0.0 for i in 1:3]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dcb1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200-element Vector{CuArray{Float32, 1, CUDA.DeviceMemory}}:\n",
       " Float32[-1.0, -1.0, 1.0, 0.0, 0.57735026]\n",
       " Float32[-1.0, -3.0, -2.0, 0.0, 0.26726124]\n",
       " Float32[-1.0, -4.0, -2.0, 0.0, 0.2182179]\n",
       " Float32[-1.0, 0.0, 2.0, 0.0, 0.4472136]\n",
       " Float32[-2.0, -2.0, 2.0, 0.0, 0.28867513]\n",
       " Float32[0.0, 4.0, 3.0, 0.0, 0.2]\n",
       " Float32[3.0, -5.0, 3.0, 0.0, 0.15249857]\n",
       " Float32[-3.0, -5.0, 5.0, 0.0, 0.13018891]\n",
       " Float32[0.0, -5.0, 1.0, 0.0, 0.19611613]\n",
       " Float32[2.0, 4.0, 4.0, 0.0, 0.16666667]\n",
       " Float32[5.0, 5.0, 2.0, 0.0, 0.13608277]\n",
       " Float32[5.0, 1.0, -4.0, 0.0, 0.15430336]\n",
       " Float32[1.0, 1.0, 3.0, 0.0, 0.30151135]\n",
       " ⋮\n",
       " Float32[-5.0, -5.0, -3.0, 0.0, 0.13018891]\n",
       " Float32[4.0, 4.0, 3.0, 0.0, 0.15617377]\n",
       " Float32[1.0, -2.0, 4.0, 0.0, 0.2182179]\n",
       " Float32[-2.0, -5.0, 4.0, 0.0, 0.1490712]\n",
       " Float32[-4.0, 4.0, 2.0, 0.0, 0.16666667]\n",
       " Float32[3.0, 4.0, -4.0, 0.0, 0.15617377]\n",
       " Float32[-4.0, -5.0, -4.0, 0.0, 0.13245323]\n",
       " Float32[-4.0, -3.0, -5.0, 0.0, 0.14142136]\n",
       " Float32[4.0, -2.0, 2.0, 0.0, 0.20412415]\n",
       " Float32[-2.0, 2.0, 2.0, 0.0, 0.28867513]\n",
       " Float32[4.0, 5.0, -1.0, 0.0, 0.15430336]\n",
       " Float32[-1.0, 2.0, 4.0, 0.0, 0.2182179]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measured_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a7fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "additional_loss_weightened (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function additional_loss_weightened(lambda)\n",
    "    \n",
    "    function additional_loss(phi_pred_fun, θ, p_)\n",
    "        CUDA.allowscalar() do\n",
    "            # phi is first output of phi_pred_fun\n",
    "            result = sum(abs2(phi_pred_fun([x, y, z, t]|>cpud, θ|>cpud)[1] - phi|>cpud) for (x, y, z, t, phi) in measured_points) / length(measured_points)|>cpud\n",
    "            result = result * lambda\n",
    "            return result\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return additional_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d660e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhysicsInformedNN{Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}, ComponentVector{Float32, CuArray{Float32, 1, CUDA.DeviceMemory}, Tuple{Axis{(layer_1 = ViewAxis(1:160, Axis(weight = ViewAxis(1:128, ShapedAxis((32, 4))), bias = ViewAxis(129:160, Shaped1DAxis((32,))))), layer_2 = ViewAxis(161:1216, Axis(weight = ViewAxis(1:1024, ShapedAxis((32, 32))), bias = ViewAxis(1025:1056, Shaped1DAxis((32,))))), layer_3 = ViewAxis(1217:2272, Axis(weight = ViewAxis(1:1024, ShapedAxis((32, 32))), bias = ViewAxis(1025:1056, Shaped1DAxis((32,))))), layer_4 = ViewAxis(2273:2536, Axis(weight = ViewAxis(1:256, ShapedAxis((8, 32))), bias = ViewAxis(257:264, Shaped1DAxis((8,))))))}}}, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, typeof(NeuralPDE.numeric_derivative), Bool, var\"#additional_loss#9\"{Int64}, Nothing, Nothing, Base.RefValue{Int64}, Base.Pairs{Symbol, Union{}, Tuple{}, @NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0), (layer_1 = (weight = Float32[0.43873644 0.5720638 -0.8534556 0.588361; -0.43472376 -0.10792436 0.5118949 -0.031731833; … ; -0.21277274 0.032084804 -0.44414806 0.69877607; -0.4684896 -0.47059092 -0.25818518 -0.7083163], bias = Float32[0.32510734, 0.07089245, 0.48741442, 0.32617354, 0.33878678, 0.115698636, 0.12690812, -0.431993, -0.09202111, 0.36180305  …  -0.36018068, -0.47118902, -0.3916126, -0.36790234, 0.21978456, -0.4440676, 0.07148701, -0.15191174, 0.4728768, 0.20038337]), layer_2 = (weight = Float32[0.305109 0.011728149 … -0.25716117 -0.023180647; -0.036541633 0.22429878 … 0.12862226 -0.27604216; … ; 0.15707372 -0.20166081 … 0.2844832 0.10532324; 0.15685391 -0.2732656 … -0.031734515 -0.047171745], bias = Float32[0.040045258, -0.011121499, 0.065936446, -0.061352093, -0.13736375, 0.05817993, 0.06048998, 0.10640311, -0.06193964, -0.059916444  …  -0.0049757096, 0.16494623, 0.06676269, 0.04628257, 0.16209547, -0.107444055, -0.1367068, -0.17330743, 0.04163466, 0.11180105]), layer_3 = (weight = Float32[0.13438141 0.23212479 … 0.27251062 -0.2955398; 0.032441925 -0.10686074 … -0.24451567 -0.09503006; … ; 0.12110478 -0.23036934 … -0.04904855 0.19023415; -0.23963697 0.106435366 … -0.20826784 0.15534227], bias = Float32[-0.06624182, -0.095770456, 0.10799203, -0.07918369, -0.015845403, -0.1325392, -0.0046769516, 0.018556036, -0.06739945, -0.14910282  …  -0.033076234, -0.08553459, -0.038971715, 0.071701206, -0.15196016, 0.13670887, 0.15056874, -0.054390635, 0.030673444, 0.15026498]), layer_4 = (weight = Float32[-0.2687029 -0.29890564 … -0.012840785 -0.007329869; -0.28946075 0.1485846 … -0.24432112 9.756515f-5; … ; 0.154684 -0.0772643 … -0.11497292 0.25088963; 0.10039508 -0.12201378 … -0.054656412 0.119655825], bias = Float32[-0.10505437, 0.06536462, -0.03577846, 0.15693052, -0.05609752, -0.101164915, 0.09227817, -0.079871505])), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), NeuralPDE.numeric_derivative, false, var\"#additional_loss#9\"{Int64}(10), nothing, nothing, LogOptions(50), Base.RefValue{Int64}(1), true, false, Base.Pairs{Symbol, Union{}, Tuple{}, @NamedTuple{}}())"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Нейросеть\n",
    "# Определение новой нейросети для моделирования распределения заряда и тока\n",
    "input_ = 4  # x, y, z\n",
    "n = 32      # число нейронов в скрытых слоях\n",
    "lambda = 10 # вес дополнительной потерь\n",
    "# Функция для разделения выхода сети на переменные\n",
    "\"\"\"\n",
    "function split_outputs(out)\n",
    "    φ_pred = out[1]\n",
    "    A_pred = out[2:4]\n",
    "    ρ_pred = out[5]\n",
    "    j_pred = out[6:8]\n",
    "    return φ_pred, A_pred, ρ_pred, j_pred\n",
    "end\n",
    "\n",
    "chain = [Chain(\n",
    "    Dense(input_, n, σ),\n",
    "    Dense(n, n, σ),\n",
    "    Dense(n, 1)\n",
    ") for _ in 1:8] \n",
    "\n",
    "# Определение системы\n",
    "ps = [Lux.setup(Random.default_rng(), chain[i])[1] |> ComponentArray |> gpud .|> Float64 for i in 1:8]\n",
    "\"\"\"\n",
    "chain = Chain(\n",
    "    Dense(input_, n, σ),\n",
    "    Dense(n, n, σ),\n",
    "    Dense(n, n, σ),\n",
    "    Dense(n, 8)\n",
    ")\n",
    "\n",
    "# Определение системы\n",
    "ps = Lux.setup(Random.default_rng(), chain)[1] |> ComponentArray |> gpud .|> Float32\n",
    "\n",
    "strategy = QuasiRandomTraining(4096)\n",
    "discretization = PhysicsInformedNN(chain, strategy; init_params = ps, additional_loss = additional_loss_weightened(lambda), \n",
    "log_options = LogOptions(; log_frequency = 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9525824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10607222f0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_loss_weightened(lambda)(discretization.phi, ps, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01addc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\begin{equation}\n",
       "\\left[\n",
       "\\begin{array}{c}\n",
       "\\varphi\\left( x, y, z, t \\right) \\\\\n",
       "\\mathtt{Ax}\\left( x, y, z, t \\right) \\\\\n",
       "\\mathtt{Ay}\\left( x, y, z, t \\right) \\\\\n",
       "\\mathtt{Az}\\left( x, y, z, t \\right) \\\\\n",
       "\\rho\\left( x, y, z, t \\right) \\\\\n",
       "\\mathtt{jx}\\left( x, y, z, t \\right) \\\\\n",
       "\\mathtt{jy}\\left( x, y, z, t \\right) \\\\\n",
       "\\mathtt{jz}\\left( x, y, z, t \\right) \\\\\n",
       "\\end{array}\n",
       "\\right]\n",
       "\\end{equation}\n",
       " $$"
      ],
      "text/plain": [
       "8-element Vector{Num}:\n",
       "  φ(x, y, z, t)\n",
       " Ax(x, y, z, t)\n",
       " Ay(x, y, z, t)\n",
       " Az(x, y, z, t)\n",
       "  ρ(x, y, z, t)\n",
       " jx(x, y, z, t)\n",
       " jy(x, y, z, t)\n",
       " jz(x, y, z, t)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allvars = [φ(x, y, z, t); [A_(x, y, z, t) for A_ in A]; ρ(x, y, z, t); [j_(x, y, z, t) for j_ in j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3610692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\begin{align}\n",
       "\\frac{\\mathrm{d}}{\\mathrm{d}y} \\frac{\\mathrm{d}}{\\mathrm{d}y} \\varphi\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{\\mathrm{d}}{\\mathrm{d}x} \\varphi\\left( x, y, z, t \\right) - 1.1127 \\cdot 10^{-21} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\varphi\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}z} \\frac{\\mathrm{d}}{\\mathrm{d}z} \\varphi\\left( x, y, z, t \\right) &=  - 12.566 \\rho\\left( x, y, z, t \\right) \\\\\n",
       "\\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{\\mathrm{d}}{\\mathrm{d}x} \\mathtt{Ax}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}z} \\frac{\\mathrm{d}}{\\mathrm{d}z} \\mathtt{Ax}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}y} \\frac{\\mathrm{d}}{\\mathrm{d}y} \\mathtt{Ax}\\left( x, y, z, t \\right) - 1.1127 \\cdot 10^{-21} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\mathtt{Ax}\\left( x, y, z, t \\right) &=  - 4.1917 \\cdot 10^{-10} \\mathtt{jx}\\left( x, y, z, t \\right) \\\\\n",
       "\\frac{\\mathrm{d}}{\\mathrm{d}y} \\frac{\\mathrm{d}}{\\mathrm{d}y} \\mathtt{Ay}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}z} \\frac{\\mathrm{d}}{\\mathrm{d}z} \\mathtt{Ay}\\left( x, y, z, t \\right) - 1.1127 \\cdot 10^{-21} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\mathtt{Ay}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{\\mathrm{d}}{\\mathrm{d}x} \\mathtt{Ay}\\left( x, y, z, t \\right) &=  - 4.1917 \\cdot 10^{-10} \\mathtt{jy}\\left( x, y, z, t \\right) \\\\\n",
       "\\frac{\\mathrm{d}}{\\mathrm{d}z} \\frac{\\mathrm{d}}{\\mathrm{d}z} \\mathtt{Az}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}y} \\frac{\\mathrm{d}}{\\mathrm{d}y} \\mathtt{Az}\\left( x, y, z, t \\right) - 1.1127 \\cdot 10^{-21} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\mathtt{Az}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}x} \\frac{\\mathrm{d}}{\\mathrm{d}x} \\mathtt{Az}\\left( x, y, z, t \\right) &=  - 4.1917 \\cdot 10^{-10} \\mathtt{jz}\\left( x, y, z, t \\right) \\\\\n",
       "3.3356 \\cdot 10^{-11} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\varphi\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}y} \\mathtt{Ay}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}z} \\mathtt{Az}\\left( x, y, z, t \\right) + \\frac{\\mathrm{d}}{\\mathrm{d}x} \\mathtt{Ax}\\left( x, y, z, t \\right) &= 0\n",
       "\\end{align}\n",
       " $$"
      ],
      "text/plain": [
       "PDESystem\n",
       "Equations: Equation[Differential(y)(Differential(y)(φ(x, y, z, t))) + Differential(x)(Differential(x)(φ(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(φ(x, y, z, t))) + Differential(z)(Differential(z)(φ(x, y, z, t))) ~ -12.566370614359172ρ(x, y, z, t), Differential(x)(Differential(x)(Ax(x, y, z, t))) + Differential(z)(Differential(z)(Ax(x, y, z, t))) + Differential(y)(Differential(y)(Ax(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(Ax(x, y, z, t))) ~ -4.191690043903363e-10jx(x, y, z, t), Differential(y)(Differential(y)(Ay(x, y, z, t))) + Differential(z)(Differential(z)(Ay(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(Ay(x, y, z, t))) + Differential(x)(Differential(x)(Ay(x, y, z, t))) ~ -4.191690043903363e-10jy(x, y, z, t), Differential(z)(Differential(z)(Az(x, y, z, t))) + Differential(y)(Differential(y)(Az(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(Az(x, y, z, t))) + Differential(x)(Differential(x)(Az(x, y, z, t))) ~ -4.191690043903363e-10jz(x, y, z, t), 3.33564095198152e-11Differential(t)(φ(x, y, z, t)) + Differential(y)(Ay(x, y, z, t)) + Differential(z)(Az(x, y, z, t)) + Differential(x)(Ax(x, y, z, t)) ~ 0.0]\n",
       "Boundary Conditions: Equation[φ(-10.0, y, z, t) ~ 0.0, φ(10.0, y, z, t) ~ 0.0, φ(x, -10.0, z, t) ~ 0.0, φ(x, 10.0, z, t) ~ 0.0, φ(x, y, -10.0, t) ~ 0.0, φ(x, y, 10.0, t) ~ 0.0, Ax(-10.0, y, z, t) ~ 0.0, Ay(-10.0, y, z, t) ~ 0.0, Az(-10.0, y, z, t) ~ 0.0]\n",
       "Domain: Symbolics.VarDomainPairing[Symbolics.VarDomainPairing(x, -10.0 .. 10.0), Symbolics.VarDomainPairing(y, -10.0 .. 10.0), Symbolics.VarDomainPairing(z, -10.0 .. 10.0), Symbolics.VarDomainPairing(t, 0.0 .. 1.0)]\n",
       "Dependent Variables: Num[φ(x, y, z, t), Ax(x, y, z, t), Ay(x, y, z, t), Az(x, y, z, t), ρ(x, y, z, t), jx(x, y, z, t), jy(x, y, z, t), jz(x, y, z, t)]\n",
       "Independent Variables: Num[x, y, z, t]\n",
       "Parameters: SciMLBase.NullParameters()\n",
       "Default Parameter ValuesDict{Any, Any}()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@named pde_system = PDESystem(eqs, bcs, domains, [x, y, z, t], allvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5817f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralPDE.PINNRepresentation(Equation[Differential(y)(Differential(y)(φ(x, y, z, t))) + Differential(x)(Differential(x)(φ(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(φ(x, y, z, t))) + Differential(z)(Differential(z)(φ(x, y, z, t))) ~ -12.566370614359172ρ(x, y, z, t), Differential(x)(Differential(x)(Ax(x, y, z, t))) + Differential(z)(Differential(z)(Ax(x, y, z, t))) + Differential(y)(Differential(y)(Ax(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(Ax(x, y, z, t))) ~ -4.191690043903363e-10jx(x, y, z, t), Differential(y)(Differential(y)(Ay(x, y, z, t))) + Differential(z)(Differential(z)(Ay(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(Ay(x, y, z, t))) + Differential(x)(Differential(x)(Ay(x, y, z, t))) ~ -4.191690043903363e-10jy(x, y, z, t), Differential(z)(Differential(z)(Az(x, y, z, t))) + Differential(y)(Differential(y)(Az(x, y, z, t))) - 1.1126500560536184e-21Differential(t)(Differential(t)(Az(x, y, z, t))) + Differential(x)(Differential(x)(Az(x, y, z, t))) ~ -4.191690043903363e-10jz(x, y, z, t), 3.33564095198152e-11Differential(t)(φ(x, y, z, t)) + Differential(y)(Ay(x, y, z, t)) + Differential(z)(Az(x, y, z, t)) + Differential(x)(Ax(x, y, z, t)) ~ 0.0], Equation[φ(-10.0, y, z, t) ~ 0.0, φ(10.0, y, z, t) ~ 0.0, φ(x, -10.0, z, t) ~ 0.0, φ(x, 10.0, z, t) ~ 0.0, φ(x, y, -10.0, t) ~ 0.0, φ(x, y, 10.0, t) ~ 0.0, Ax(-10.0, y, z, t) ~ 0.0, Ay(-10.0, y, z, t) ~ 0.0, Az(-10.0, y, z, t) ~ 0.0], Symbolics.VarDomainPairing[Symbolics.VarDomainPairing(x, -10.0 .. 10.0), Symbolics.VarDomainPairing(y, -10.0 .. 10.0), Symbolics.VarDomainPairing(z, -10.0 .. 10.0), Symbolics.VarDomainPairing(t, 0.0 .. 1.0)], SciMLBase.NullParameters(), Dict{Any, Any}(), nothing, false, var\"#additional_loss#9\"{Int64}(10), NonAdaptiveLoss{Float32}(Float32[1.0, 1.0, 1.0, 1.0, 1.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[1.0]), [:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz], [:x, :y, :z, :t], Dict(:y => 2, :z => 3, :t => 4, :x => 1), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:φ => [:x, :y, :z, :t], :ρ => [:x, :y, :z, :t], :Az => [:x, :y, :z, :t], :jx => [:x, :y, :z, :t], :Ax => [:x, :y, :z, :t], :jz => [:x, :y, :z, :t], :jy => [:x, :y, :z, :t], :Ay => [:x, :y, :z, :t]), nothing, false, Base.RefValue{Int64}(1), (layer_1 = (weight = Float32[0.43873644 0.5720638 -0.8534556 0.588361; -0.43472376 -0.10792436 0.5118949 -0.031731833; … ; -0.21277274 0.032084804 -0.44414806 0.69877607; -0.4684896 -0.47059092 -0.25818518 -0.7083163], bias = Float32[0.32510734, 0.07089245, 0.48741442, 0.32617354, 0.33878678, 0.115698636, 0.12690812, -0.431993, -0.09202111, 0.36180305  …  -0.36018068, -0.47118902, -0.3916126, -0.36790234, 0.21978456, -0.4440676, 0.07148701, -0.15191174, 0.4728768, 0.20038337]), layer_2 = (weight = Float32[0.305109 0.011728149 … -0.25716117 -0.023180647; -0.036541633 0.22429878 … 0.12862226 -0.27604216; … ; 0.15707372 -0.20166081 … 0.2844832 0.10532324; 0.15685391 -0.2732656 … -0.031734515 -0.047171745], bias = Float32[0.040045258, -0.011121499, 0.065936446, -0.061352093, -0.13736375, 0.05817993, 0.06048998, 0.10640311, -0.06193964, -0.059916444  …  -0.0049757096, 0.16494623, 0.06676269, 0.04628257, 0.16209547, -0.107444055, -0.1367068, -0.17330743, 0.04163466, 0.11180105]), layer_3 = (weight = Float32[0.13438141 0.23212479 … 0.27251062 -0.2955398; 0.032441925 -0.10686074 … -0.24451567 -0.09503006; … ; 0.12110478 -0.23036934 … -0.04904855 0.19023415; -0.23963697 0.106435366 … -0.20826784 0.15534227], bias = Float32[-0.06624182, -0.095770456, 0.10799203, -0.07918369, -0.015845403, -0.1325392, -0.0046769516, 0.018556036, -0.06739945, -0.14910282  …  -0.033076234, -0.08553459, -0.038971715, 0.071701206, -0.15196016, 0.13670887, 0.15056874, -0.054390635, 0.030673444, 0.15026498]), layer_4 = (weight = Float32[-0.2687029 -0.29890564 … -0.012840785 -0.007329869; -0.28946075 0.1485846 … -0.24432112 9.756515f-5; … ; 0.154684 -0.0772643 … -0.11497292 0.25088963; 0.10039508 -0.12201378 … -0.054656412 0.119655825], bias = Float32[-0.10505437, 0.06536462, -0.03577846, 0.15693052, -0.05609752, -0.101164915, 0.09227817, -0.079871505])), (layer_1 = (weight = Float32[0.43873644 0.5720638 -0.8534556 0.588361; -0.43472376 -0.10792436 0.5118949 -0.031731833; … ; -0.21277274 0.032084804 -0.44414806 0.69877607; -0.4684896 -0.47059092 -0.25818518 -0.7083163], bias = Float32[0.32510734, 0.07089245, 0.48741442, 0.32617354, 0.33878678, 0.115698636, 0.12690812, -0.431993, -0.09202111, 0.36180305  …  -0.36018068, -0.47118902, -0.3916126, -0.36790234, 0.21978456, -0.4440676, 0.07148701, -0.15191174, 0.4728768, 0.20038337]), layer_2 = (weight = Float32[0.305109 0.011728149 … -0.25716117 -0.023180647; -0.036541633 0.22429878 … 0.12862226 -0.27604216; … ; 0.15707372 -0.20166081 … 0.2844832 0.10532324; 0.15685391 -0.2732656 … -0.031734515 -0.047171745], bias = Float32[0.040045258, -0.011121499, 0.065936446, -0.061352093, -0.13736375, 0.05817993, 0.06048998, 0.10640311, -0.06193964, -0.059916444  …  -0.0049757096, 0.16494623, 0.06676269, 0.04628257, 0.16209547, -0.107444055, -0.1367068, -0.17330743, 0.04163466, 0.11180105]), layer_3 = (weight = Float32[0.13438141 0.23212479 … 0.27251062 -0.2955398; 0.032441925 -0.10686074 … -0.24451567 -0.09503006; … ; 0.12110478 -0.23036934 … -0.04904855 0.19023415; -0.23963697 0.106435366 … -0.20826784 0.15534227], bias = Float32[-0.06624182, -0.095770456, 0.10799203, -0.07918369, -0.015845403, -0.1325392, -0.0046769516, 0.018556036, -0.06739945, -0.14910282  …  -0.033076234, -0.08553459, -0.038971715, 0.071701206, -0.15196016, 0.13670887, 0.15056874, -0.054390635, 0.030673444, 0.15026498]), layer_4 = (weight = Float32[-0.2687029 -0.29890564 … -0.012840785 -0.007329869; -0.28946075 0.1485846 … -0.24432112 9.756515f-5; … ; 0.154684 -0.0772643 … -0.11497292 0.25088963; 0.10039508 -0.12201378 … -0.054656412 0.119655825], bias = Float32[-0.10505437, 0.06536462, -0.03577846, 0.15693052, -0.05609752, -0.101164915, 0.09227817, -0.079871505])), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), NeuralPDE.numeric_derivative, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0), Vector{Any}[[:x, :y, :z, :t], [:x, :y, :z, :t], [:x, :y, :z, :t], [:x, :y, :z, :t], [:x, :y, :z, :t]], Vector{Any}[[:y, :z, :t], [:y, :z, :t], [:x, :z, :t], [:x, :z, :t], [:x, :y, :t], [:x, :y, :t], [:y, :z, :t], [:y, :z, :t], [:y, :z, :t]], [[:t, :x, :y, :z], [:t, :x, :y, :z], [:t, :x, :y, :z], [:t, :x, :y, :z], [:t, :x, :y, :z]], [[:t, :y, :z], [:t, :y, :z], [:t, :x, :z], [:t, :x, :z], [:t, :x, :y], [:t, :x, :y], [:t, :y, :z], [:t, :y, :z], [:t, :y, :z]], NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), Expr[:((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord5 = vcat(x, y, z, t)\n",
       "                      cord1 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  (+).((+).((+).(derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord1, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))), derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\")) .- (*).(-12.566370614359172, u(cord5, var\"##θ#230\", phi))\n",
       "              end\n",
       "          end\n",
       "      end), :((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord6 = vcat(x, y, z, t)\n",
       "                      cord2 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  (+).((+).((+).(derivative(phi, u, cord2, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord2, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\")), derivative(phi, u, cord2, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord2, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))) .- (*).(-4.191690043903363e-10, u(cord6, var\"##θ#230\", phi))\n",
       "              end\n",
       "          end\n",
       "      end), :((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord7 = vcat(x, y, z, t)\n",
       "                      cord3 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  (+).((+).((+).(derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))), derivative(phi, u, cord3, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\")) .- (*).(-4.191690043903363e-10, u(cord7, var\"##θ#230\", phi))\n",
       "              end\n",
       "          end\n",
       "      end), :((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord4 = vcat(x, y, z, t)\n",
       "                      cord8 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  (+).((+).((+).(derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))), derivative(phi, u, cord4, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\")) .- (*).(-4.191690043903363e-10, u(cord8, var\"##θ#230\", phi))\n",
       "              end\n",
       "          end\n",
       "      end), :((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord4 = vcat(x, y, z, t)\n",
       "                      cord2 = vcat(x, y, z, t)\n",
       "                      cord3 = vcat(x, y, z, t)\n",
       "                      cord1 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  (+).((+).((+).((*).(3.33564095198152e-11, derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.0, 0.0, 0.0049215658]], 1, var\"##θ#230\")), derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.0049215658, 0.0, 0.0]], 1, var\"##θ#230\")), derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.0, 0.0049215658, 0.0]], 1, var\"##θ#230\")), derivative(phi, u, cord2, Vector{Float32}[[0.0049215658, 0.0, 0.0, 0.0]], 1, var\"##θ#230\")) .- 0.0\n",
       "              end\n",
       "          end\n",
       "      end)], Expr[:((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord1 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "              end\n",
       "          end\n",
       "      end), :((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord1 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "              end\n",
       "          end\n",
       "      end), :((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord1 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "              end\n",
       "          end\n",
       "      end), :((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord1 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "              end\n",
       "          end\n",
       "      end), :((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord1 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "              end\n",
       "          end\n",
       "      end), :((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord1 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "              end\n",
       "          end\n",
       "      end), :((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord2 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  u(cord2, var\"##θ#230\", phi) .- 0.0\n",
       "              end\n",
       "          end\n",
       "      end), :((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord3 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  u(cord3, var\"##θ#230\", phi) .- 0.0\n",
       "              end\n",
       "          end\n",
       "      end), :((cord, var\"##θ#230\", phi, derivative, integral, u, p)->begin\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "          \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "          begin\n",
       "              let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "                  begin\n",
       "                      cord4 = vcat(x, y, z, t)\n",
       "                  end\n",
       "                  u(cord4, var\"##θ#230\", phi) .- 0.0\n",
       "              end\n",
       "          end\n",
       "      end)], NeuralPDE.PINNLossFunctions(NeuralPDE.var\"#94#97\"{loss_function, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample} where loss_function[NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-10.0, -9.999756, -9.999756, 0.00024414062], Float32[-10.0, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[10.0, -9.999756, -9.999756, 0.00024414062], Float32[10.0, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -10.0, -9.999756, 0.00024414062], Float32[9.999756, -10.0, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, 10.0, -9.999756, 0.00024414062], Float32[9.999756, 10.0, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, -10.0, 0.00024414062], Float32[9.999756, 9.999756, -10.0, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, 10.0, 0.00024414062], Float32[9.999756, 9.999756, 10.0, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x66f13037, 0x5fcc5051, 0x386ee0de, 0x37396a0f, 0x7884cdc3), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x66f13037, 0x5fcc5051, 0x386ee0de, 0x37396a0f, 0x7884cdc3), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x66f13037, 0x5fcc5051, 0x386ee0de, 0x37396a0f, 0x7884cdc3), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord2 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord2, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-10.0, -9.999756, -9.999756, 0.00024414062], Float32[-10.0, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x2e9fb24d, 0x0fe98539, 0x436aa1a0, 0xfcc34bb7, 0x9e3c00f0), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x2e9fb24d, 0x0fe98539, 0x436aa1a0, 0xfcc34bb7, 0x9e3c00f0), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x2e9fb24d, 0x0fe98539, 0x436aa1a0, 0xfcc34bb7, 0x9e3c00f0), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord3 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord3, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-10.0, -9.999756, -9.999756, 0.00024414062], Float32[-10.0, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6ab35d25, 0xd5c687c0, 0x3b8586db, 0x3474c683, 0x3951f605), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6ab35d25, 0xd5c687c0, 0x3b8586db, 0x3474c683, 0x3951f605), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6ab35d25, 0xd5c687c0, 0x3b8586db, 0x3474c683, 0x3951f605), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord4 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord4, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-10.0, -9.999756, -9.999756, 0.00024414062], Float32[-10.0, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()))], NeuralPDE.var\"#94#97\"{loss_function, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample} where loss_function[NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x5b2b209c, 0x0868d5a4, 0x3d4cef40, 0xc2f3327c, 0xac51a340), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x5b2b209c, 0x0868d5a4, 0x3d4cef40, 0xc2f3327c, 0xac51a340), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x5b2b209c, 0x0868d5a4, 0x3d4cef40, 0xc2f3327c, 0xac51a340), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord5 = vcat(x, y, z, t)\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).(derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord1, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))), derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\")) .- (*).(-12.566370614359172, u(cord5, var\"##θ#230\", phi))\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, -9.999756, 0.00024414062], Float32[9.999756, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x85f2ff1b, 0xb10a869a, 0x92ea17c2, 0x855ffd32, 0x095d9a2e), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x85f2ff1b, 0xb10a869a, 0x92ea17c2, 0x855ffd32, 0x095d9a2e), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x85f2ff1b, 0xb10a869a, 0x92ea17c2, 0x855ffd32, 0x095d9a2e), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord6 = vcat(x, y, z, t)\n",
       "                cord2 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).(derivative(phi, u, cord2, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord2, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\")), derivative(phi, u, cord2, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord2, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))) .- (*).(-4.191690043903363e-10, u(cord6, var\"##θ#230\", phi))\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, -9.999756, 0.00024414062], Float32[9.999756, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x1d17a2af, 0x1a25e961, 0xa221385d, 0x45a9b2fb, 0xdb8b3693), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x1d17a2af, 0x1a25e961, 0xa221385d, 0x45a9b2fb, 0xdb8b3693), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x1d17a2af, 0x1a25e961, 0xa221385d, 0x45a9b2fb, 0xdb8b3693), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord7 = vcat(x, y, z, t)\n",
       "                cord3 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).(derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))), derivative(phi, u, cord3, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\")) .- (*).(-4.191690043903363e-10, u(cord7, var\"##θ#230\", phi))\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, -9.999756, 0.00024414062], Float32[9.999756, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6a03b19c, 0xe4514e31, 0x714be584, 0x478ea9fc, 0x9607dd93), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6a03b19c, 0xe4514e31, 0x714be584, 0x478ea9fc, 0x9607dd93), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6a03b19c, 0xe4514e31, 0x714be584, 0x478ea9fc, 0x9607dd93), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord4 = vcat(x, y, z, t)\n",
       "                cord8 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).(derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))), derivative(phi, u, cord4, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\")) .- (*).(-4.191690043903363e-10, u(cord8, var\"##θ#230\", phi))\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, -9.999756, 0.00024414062], Float32[9.999756, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0xef957cc8, 0x7d27359e, 0x5ae49349, 0x1767cf17, 0x4e1429e5), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0xef957cc8, 0x7d27359e, 0x5ae49349, 0x1767cf17, 0x4e1429e5), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0xef957cc8, 0x7d27359e, 0x5ae49349, 0x1767cf17, 0x4e1429e5), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord4 = vcat(x, y, z, t)\n",
       "                cord2 = vcat(x, y, z, t)\n",
       "                cord3 = vcat(x, y, z, t)\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).((*).(3.33564095198152e-11, derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.0, 0.0, 0.0049215658]], 1, var\"##θ#230\")), derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.0049215658, 0.0, 0.0]], 1, var\"##θ#230\")), derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.0, 0.0049215658, 0.0]], 1, var\"##θ#230\")), derivative(phi, u, cord2, Vector{Float32}[[0.0049215658, 0.0, 0.0, 0.0]], 1, var\"##θ#230\")) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, -9.999756, 0.00024414062], Float32[9.999756, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()))], NeuralPDE.var\"#full_loss_function#283\"{Returns{Nothing}, Vector{NeuralPDE.var\"#94#97\"{loss_function, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample} where loss_function}, Vector{NeuralPDE.var\"#94#97\"{loss_function, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample} where loss_function}, NeuralPDE.PINNRepresentation, Int64, Bool, Base.RefValue{Int64}, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, var\"#additional_loss#9\"{Int64}, Bool}(Returns{Nothing}(nothing), NeuralPDE.var\"#94#97\"{loss_function, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample} where loss_function[NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-10.0, -9.999756, -9.999756, 0.00024414062], Float32[-10.0, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[10.0, -9.999756, -9.999756, 0.00024414062], Float32[10.0, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -10.0, -9.999756, 0.00024414062], Float32[9.999756, -10.0, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, 10.0, -9.999756, 0.00024414062], Float32[9.999756, 10.0, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, -10.0, 0.00024414062], Float32[9.999756, 9.999756, -10.0, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, 10.0, 0.00024414062], Float32[9.999756, 9.999756, 10.0, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x66f13037, 0x5fcc5051, 0x386ee0de, 0x37396a0f, 0x7884cdc3), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x66f13037, 0x5fcc5051, 0x386ee0de, 0x37396a0f, 0x7884cdc3), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x66f13037, 0x5fcc5051, 0x386ee0de, 0x37396a0f, 0x7884cdc3), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord2 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord2, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-10.0, -9.999756, -9.999756, 0.00024414062], Float32[-10.0, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x2e9fb24d, 0x0fe98539, 0x436aa1a0, 0xfcc34bb7, 0x9e3c00f0), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x2e9fb24d, 0x0fe98539, 0x436aa1a0, 0xfcc34bb7, 0x9e3c00f0), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x2e9fb24d, 0x0fe98539, 0x436aa1a0, 0xfcc34bb7, 0x9e3c00f0), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord3 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord3, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-10.0, -9.999756, -9.999756, 0.00024414062], Float32[-10.0, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6ab35d25, 0xd5c687c0, 0x3b8586db, 0x3474c683, 0x3951f605), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6ab35d25, 0xd5c687c0, 0x3b8586db, 0x3474c683, 0x3951f605), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6ab35d25, 0xd5c687c0, 0x3b8586db, 0x3474c683, 0x3951f605), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord4 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord4, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-10.0, -9.999756, -9.999756, 0.00024414062], Float32[-10.0, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()))], NeuralPDE.var\"#94#97\"{loss_function, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample} where loss_function[NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x5b2b209c, 0x0868d5a4, 0x3d4cef40, 0xc2f3327c, 0xac51a340), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x5b2b209c, 0x0868d5a4, 0x3d4cef40, 0xc2f3327c, 0xac51a340), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x5b2b209c, 0x0868d5a4, 0x3d4cef40, 0xc2f3327c, 0xac51a340), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord5 = vcat(x, y, z, t)\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).(derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord1, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))), derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\")) .- (*).(-12.566370614359172, u(cord5, var\"##θ#230\", phi))\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, -9.999756, 0.00024414062], Float32[9.999756, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x85f2ff1b, 0xb10a869a, 0x92ea17c2, 0x855ffd32, 0x095d9a2e), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x85f2ff1b, 0xb10a869a, 0x92ea17c2, 0x855ffd32, 0x095d9a2e), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x85f2ff1b, 0xb10a869a, 0x92ea17c2, 0x855ffd32, 0x095d9a2e), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord6 = vcat(x, y, z, t)\n",
       "                cord2 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).(derivative(phi, u, cord2, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord2, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\")), derivative(phi, u, cord2, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord2, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))) .- (*).(-4.191690043903363e-10, u(cord6, var\"##θ#230\", phi))\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, -9.999756, 0.00024414062], Float32[9.999756, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x1d17a2af, 0x1a25e961, 0xa221385d, 0x45a9b2fb, 0xdb8b3693), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x1d17a2af, 0x1a25e961, 0xa221385d, 0x45a9b2fb, 0xdb8b3693), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x1d17a2af, 0x1a25e961, 0xa221385d, 0x45a9b2fb, 0xdb8b3693), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord7 = vcat(x, y, z, t)\n",
       "                cord3 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).(derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))), derivative(phi, u, cord3, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\")) .- (*).(-4.191690043903363e-10, u(cord7, var\"##θ#230\", phi))\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, -9.999756, 0.00024414062], Float32[9.999756, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6a03b19c, 0xe4514e31, 0x714be584, 0x478ea9fc, 0x9607dd93), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6a03b19c, 0xe4514e31, 0x714be584, 0x478ea9fc, 0x9607dd93), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6a03b19c, 0xe4514e31, 0x714be584, 0x478ea9fc, 0x9607dd93), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord4 = vcat(x, y, z, t)\n",
       "                cord8 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).(derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))), derivative(phi, u, cord4, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\")) .- (*).(-4.191690043903363e-10, u(cord8, var\"##θ#230\", phi))\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, -9.999756, 0.00024414062], Float32[9.999756, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG())), NeuralPDE.var\"#94#97\"{NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0xef957cc8, 0x7d27359e, 0x5ae49349, 0x1767cf17, 0x4e1429e5), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}, Tuple{Vector{Float32}, Vector{Float32}}, DataType, CUDADevice{CuDevice}, Int64, QuasiMonteCarlo.LatinHypercubeSample}(NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0xef957cc8, 0x7d27359e, 0x5ae49349, 0x1767cf17, 0x4e1429e5), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0xef957cc8, 0x7d27359e, 0x5ae49349, 0x1767cf17, 0x4e1429e5), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord4 = vcat(x, y, z, t)\n",
       "                cord2 = vcat(x, y, z, t)\n",
       "                cord3 = vcat(x, y, z, t)\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).((*).(3.33564095198152e-11, derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.0, 0.0, 0.0049215658]], 1, var\"##θ#230\")), derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.0049215658, 0.0, 0.0]], 1, var\"##θ#230\")), derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.0, 0.0049215658, 0.0]], 1, var\"##θ#230\")), derivative(phi, u, cord2, Vector{Float32}[[0.0049215658, 0.0, 0.0, 0.0]], 1, var\"##θ#230\")) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), (Float32[-9.999756, -9.999756, -9.999756, 0.00024414062], Float32[9.999756, 9.999756, 9.999756, 0.99975586]), Float32, CUDADevice{CuDevice}(CuDevice(0)), 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()))], NeuralPDE.PINNRepresentation(#= circular reference @-3 =#), Core.Box(NonAdaptiveLoss{Float32}(Float32[1.0, 1.0, 1.0, 1.0, 1.0], Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], Float32[1.0])), 50, true, Base.RefValue{Int64}(1), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), var\"#additional_loss#9\"{Int64}(10), false), var\"#additional_loss#9\"{Int64}(10), NeuralPDE.var\"#197#198\"{_loss_function, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing} where _loss_function[NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x5b2b209c, 0x0868d5a4, 0x3d4cef40, 0xc2f3327c, 0xac51a340), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x5b2b209c, 0x0868d5a4, 0x3d4cef40, 0xc2f3327c, 0xac51a340), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord5 = vcat(x, y, z, t)\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).(derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord1, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))), derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\")) .- (*).(-12.566370614359172, u(cord5, var\"##θ#230\", phi))\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x85f2ff1b, 0xb10a869a, 0x92ea17c2, 0x855ffd32, 0x095d9a2e), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x85f2ff1b, 0xb10a869a, 0x92ea17c2, 0x855ffd32, 0x095d9a2e), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord6 = vcat(x, y, z, t)\n",
       "                cord2 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).(derivative(phi, u, cord2, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord2, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\")), derivative(phi, u, cord2, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord2, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))) .- (*).(-4.191690043903363e-10, u(cord6, var\"##θ#230\", phi))\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x1d17a2af, 0x1a25e961, 0xa221385d, 0x45a9b2fb, 0xdb8b3693), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x1d17a2af, 0x1a25e961, 0xa221385d, 0x45a9b2fb, 0xdb8b3693), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord7 = vcat(x, y, z, t)\n",
       "                cord3 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).(derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))), derivative(phi, u, cord3, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\")) .- (*).(-4.191690043903363e-10, u(cord7, var\"##θ#230\", phi))\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6a03b19c, 0xe4514e31, 0x714be584, 0x478ea9fc, 0x9607dd93), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6a03b19c, 0xe4514e31, 0x714be584, 0x478ea9fc, 0x9607dd93), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord4 = vcat(x, y, z, t)\n",
       "                cord8 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).(derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.0, 0.01858136, 0.0], [0.0, 0.0, 0.01858136, 0.0]], 2, var\"##θ#230\"), derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.01858136, 0.0, 0.0], [0.0, 0.01858136, 0.0, 0.0]], 2, var\"##θ#230\")), (*).(-1.1126500560536184e-21, derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.0, 0.0, 0.01858136], [0.0, 0.0, 0.0, 0.01858136]], 2, var\"##θ#230\"))), derivative(phi, u, cord4, Vector{Float32}[[0.01858136, 0.0, 0.0, 0.0], [0.01858136, 0.0, 0.0, 0.0]], 2, var\"##θ#230\")) .- (*).(-4.191690043903363e-10, u(cord8, var\"##θ#230\", phi))\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0xef957cc8, 0x7d27359e, 0x5ae49349, 0x1767cf17, 0x4e1429e5), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0xef957cc8, 0x7d27359e, 0x5ae49349, 0x1767cf17, 0x4e1429e5), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord4 = vcat(x, y, z, t)\n",
       "                cord2 = vcat(x, y, z, t)\n",
       "                cord3 = vcat(x, y, z, t)\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            (+).((+).((+).((*).(3.33564095198152e-11, derivative(phi, u, cord1, Vector{Float32}[[0.0, 0.0, 0.0, 0.0049215658]], 1, var\"##θ#230\")), derivative(phi, u, cord3, Vector{Float32}[[0.0, 0.0049215658, 0.0, 0.0]], 1, var\"##θ#230\")), derivative(phi, u, cord4, Vector{Float32}[[0.0, 0.0, 0.0049215658, 0.0]], 1, var\"##θ#230\")), derivative(phi, u, cord2, Vector{Float32}[[0.0049215658, 0.0, 0.0, 0.0]], 1, var\"##θ#230\")) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing)], NeuralPDE.var\"#197#198\"{_loss_function, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing} where _loss_function[NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x3ff27045, 0xa0fd0e28, 0x1240535f, 0xe02ab7e0, 0x205a006c), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord1 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord1, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x66f13037, 0x5fcc5051, 0x386ee0de, 0x37396a0f, 0x7884cdc3), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x66f13037, 0x5fcc5051, 0x386ee0de, 0x37396a0f, 0x7884cdc3), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord2 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord2, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x2e9fb24d, 0x0fe98539, 0x436aa1a0, 0xfcc34bb7, 0x9e3c00f0), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x2e9fb24d, 0x0fe98539, 0x436aa1a0, 0xfcc34bb7, 0x9e3c00f0), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord3 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord3, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing), NeuralPDE.var\"#197#198\"{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6ab35d25, 0xd5c687c0, 0x3b8586db, 0x3474c683, 0x3951f605), Expr}, NeuralPDE.var\"#7#8\", NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}, typeof(NeuralPDE.numeric_derivative), NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, Nothing}(RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:cord, Symbol(\"##θ#230\"), :phi, :derivative, :integral, :u, :p), NeuralPDE.var\"#_RGF_ModTag\", NeuralPDE.var\"#_RGF_ModTag\", (0x6ab35d25, 0xd5c687c0, 0x3b8586db, 0x3474c683, 0x3951f605), Expr}(quote\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:129 =#\u001b[39m\n",
       "    \u001b[90m#= /home/sasha/.julia/packages/NeuralPDE/nYBAW/src/discretize.jl:130 =#\u001b[39m\n",
       "    begin\n",
       "        let (x, y, z, t) = (cord[[1], :], cord[[2], :], cord[[3], :], cord[[4], :])\n",
       "            begin\n",
       "                cord4 = vcat(x, y, z, t)\n",
       "            end\n",
       "            u(cord4, var\"##θ#230\", phi) .- 0.0\n",
       "        end\n",
       "    end\n",
       "end), NeuralPDE.var\"#7#8\"(), NeuralPDE.var\"#239#246\"{NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}, Dict{Symbol, Int64}, Dict{Symbol, Int64}, QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}}(NeuralPDE.var\"#239#240#247\"{typeof(NeuralPDE.numeric_derivative)}(NeuralPDE.numeric_derivative), Dict(:φ => 1, :ρ => 5, :Az => 4, :jx => 6, :Ax => 2, :jz => 8, :jy => 7, :Ay => 3), Dict(:y => 2, :z => 3, :t => 4, :x => 1), Core.Box([:φ, :Ax, :Ay, :Az, :ρ, :jx, :jy, :jz]), Core.Box([:x, :y, :z, :t]), QuasiRandomTraining{QuasiMonteCarlo.LatinHypercubeSample}(4096, 4096, QuasiMonteCarlo.LatinHypercubeSample(TaskLocalRNG()), true, 0)), NeuralPDE.numeric_derivative, NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true))), nothing)]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = discretize(pde_system, discretization)\n",
    "sym_prob = symbolic_discretize(pde_system, discretization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21235add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_callback (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#phi = discretization.phi\n",
    "pde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\n",
    "bcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n",
    "add_functon = sym_prob.loss_functions.additional_loss_function\n",
    "\n",
    "\n",
    "\n",
    "# Создаем логгер для TensorBoard\n",
    "logger = TBLogger(\"../../logs/inverse_npde_exp\")\n",
    "\n",
    "function create_callback(maxiters)\n",
    "    iter = 0  # Локальный счетчик итераций\n",
    "    pbar = ProgressBar(1:maxiters, printing_delay=1.)\n",
    "    return function (p, l)\n",
    "        \n",
    "        iter += 1  # Увеличиваем номер итерации\n",
    "        \n",
    "        # Логируем общую потерю\n",
    "        log_value(logger, \"Loss/Total\", l; step=iter)\n",
    "        \n",
    "        # Логируем потери по PDE\n",
    "        pde_losses = map(l_ -> l_(p.u), pde_inner_loss_functions)\n",
    "        for (i, pde_loss) in enumerate(pde_losses)\n",
    "            log_value(logger, \"Loss/PDE_$i\", pde_loss; step=iter)\n",
    "        end\n",
    "        \n",
    "        # Логируем потери по граничным условиям\n",
    "        bcs_losses = map(l_ -> l_(p.u), bcs_inner_loss_functions)\n",
    "        for (i, bc_loss) in enumerate(bcs_losses)\n",
    "            log_value(logger, \"Loss/BC_$i\", bc_loss; step=iter)\n",
    "        end\n",
    "        \n",
    "        # Обновляем прогресс бар\n",
    "        ProgressBars.update(pbar)  \n",
    "        set_postfix(pbar,Loss = @sprintf(\"%.3f\", l|>Float32), \n",
    "                     PDE_losses=@sprintf(\"%.3f\",sum(pde_losses)|>Float32), BC_losses=@sprintf(\"%.3f\",sum(bcs_losses)|>Float32))\n",
    "        \n",
    "        return false\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d8b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam(0.001, (0.9, 0.999), 1.0e-8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оптимизация\n",
    "opt = OptimizationOptimisers.Adam(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f985ef4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%┣                                         ┫ 1/3.0k [01:55<Inf:Inf, InfGs/it]\n",
      "0.1%┣┫ 2/3.0k [01:57<97:32:29, 117s/it, Loss: 39.717, PDE_losses: 37.527, BC_losses: 2.109]\n",
      "0.2%┣┫ 5/3.0k [01:58<24:38:18, 30s/it, Loss: 28.952, PDE_losses: 27.214, BC_losses: 1.526]\n",
      "0.3%┣┫ 8/3.0k [02:00<14:13:26, 17s/it, Loss: 20.307, PDE_losses: 18.925, BC_losses: 1.057]\n",
      "0.4%┣┫ 11/3.0k [02:01<10:04:03, 12s/it, Loss: 13.536, PDE_losses: 12.453, BC_losses: 0.692]\n",
      "0.4%┣┫ 13/3.0k [02:02<08:27:19, 10s/it, Loss: 9.952, PDE_losses: 9.069, BC_losses: 0.501]\n",
      "0.5%┣┫ 15/3.0k [02:03<07:18:13, 9s/it, Loss: 7.077, PDE_losses: 6.363, BC_losses: 0.349]\n",
      "0.6%┣┫ 18/3.0k [02:05<06:04:14, 7s/it, Loss: 3.936, PDE_losses: 3.449, BC_losses: 0.186]\n",
      "0.7%┣┫ 21/3.0k [02:06<05:12:47, 6s/it, Loss: 2.003, PDE_losses: 1.661, BC_losses: 0.087]\n",
      "0.8%┣┫ 24/3.0k [02:07<04:34:42, 6s/it, Loss: 0.966, PDE_losses: 0.691, BC_losses: 0.035]\n",
      "0.9%┣┫ 27/3.0k [02:09<04:05:02, 5s/it, Loss: 0.551, PDE_losses: 0.287, BC_losses: 0.015]\n",
      "1.0%┣┫ 30/3.0k [02:10<03:41:33, 4s/it, Loss: 0.518, PDE_losses: 0.230, BC_losses: 0.014]\n",
      "1.1%┣┫ 33/3.0k [02:11<03:22:23, 4s/it, Loss: 0.653, PDE_losses: 0.335, BC_losses: 0.021]\n",
      "1.2%┣┫ 36/3.0k [02:12<03:06:35, 4s/it, Loss: 0.798, PDE_losses: 0.466, BC_losses: 0.029]\n",
      "1.3%┣┫ 39/3.0k [02:13<02:53:17, 4s/it, Loss: 0.871, PDE_losses: 0.549, BC_losses: 0.034]\n",
      "1.4%┣┫ 42/3.0k [02:14<02:41:44, 3s/it, Loss: 0.860, PDE_losses: 0.554, BC_losses: 0.034]\n",
      "1.5%┣┫ 45/3.0k [02:16<02:31:46, 3s/it, Loss: 0.788, PDE_losses: 0.494, BC_losses: 0.031]\n",
      "1.6%┣┫ 48/3.0k [02:17<02:23:17, 3s/it, Loss: 0.686, PDE_losses: 0.395, BC_losses: 0.025]\n",
      "1.7%┣┫ 51/3.0k [02:19<02:16:35, 3s/it, Loss: 0.587, PDE_losses: 0.292, BC_losses: 0.018]\n",
      "1.8%┣┫ 54/3.0k [02:20<02:09:58, 3s/it, Loss: 0.508, PDE_losses: 0.209, BC_losses: 0.013]\n",
      "1.9%┣┫ 56/3.0k [02:21<02:06:04, 3s/it, Loss: 0.471, PDE_losses: 0.173, BC_losses: 0.010]\n",
      "2.0%┣┫ 59/3.0k [02:23<02:00:26, 2s/it, Loss: 0.437, PDE_losses: 0.143, BC_losses: 0.008]\n",
      "2.1%┣┫ 62/3.0k [02:24<01:55:28, 2s/it, Loss: 0.423, PDE_losses: 0.137, BC_losses: 0.007]\n",
      "2.2%┣┫ 65/3.0k [02:25<01:50:59, 2s/it, Loss: 0.422, PDE_losses: 0.141, BC_losses: 0.007]\n",
      "2.2%┣┫ 67/3.0k [02:26<01:48:22, 2s/it, Loss: 0.424, PDE_losses: 0.145, BC_losses: 0.007]\n",
      "2.3%┣┫ 70/3.0k [02:28<01:44:32, 2s/it, Loss: 0.428, PDE_losses: 0.146, BC_losses: 0.007]\n",
      "2.4%┣┫ 73/3.0k [02:29<01:41:05, 2s/it, Loss: 0.430, PDE_losses: 0.143, BC_losses: 0.007]\n",
      "2.5%┣┫ 76/3.0k [02:31<01:37:49, 2s/it, Loss: 0.427, PDE_losses: 0.139, BC_losses: 0.007]\n",
      "2.6%┣┫ 78/3.0k [02:32<01:35:51, 2s/it, Loss: 0.423, PDE_losses: 0.137, BC_losses: 0.007]\n",
      "2.7%┣┫ 81/3.0k [02:33<01:33:00, 2s/it, Loss: 0.417, PDE_losses: 0.133, BC_losses: 0.007]\n",
      "2.8%┣┫ 84/3.0k [02:34<01:30:15, 2s/it, Loss: 0.412, PDE_losses: 0.130, BC_losses: 0.007]\n",
      "2.9%┣┫ 87/3.0k [02:35<01:27:40, 2s/it, Loss: 0.408, PDE_losses: 0.126, BC_losses: 0.007]\n",
      "3.0%┣┫ 90/3.0k [02:36<01:25:12, 2s/it, Loss: 0.405, PDE_losses: 0.121, BC_losses: 0.007]\n",
      "3.1%┣┫ 93/3.0k [02:38<01:22:58, 2s/it, Loss: 0.403, PDE_losses: 0.120, BC_losses: 0.006]\n",
      "3.2%┣┫ 96/3.0k [02:39<01:20:53, 2s/it, Loss: 0.403, PDE_losses: 0.119, BC_losses: 0.006]\n",
      "3.3%┣┫ 99/3.0k [02:40<01:18:59, 2s/it, Loss: 0.402, PDE_losses: 0.119, BC_losses: 0.007]\n",
      "3.4%┣┫ 102/3.0k [02:41<01:17:09, 2s/it, Loss: 0.401, PDE_losses: 0.119, BC_losses: 0.007]\n",
      "3.5%┣┫ 105/3.0k [02:43<01:15:27, 2s/it, Loss: 0.400, PDE_losses: 0.118, BC_losses: 0.006]\n",
      "3.6%┣┫ 108/3.0k [02:44<01:13:49, 2s/it, Loss: 0.399, PDE_losses: 0.116, BC_losses: 0.006]\n",
      "3.7%┣┫ 111/3.0k [02:45<01:12:17, 2s/it, Loss: 0.397, PDE_losses: 0.115, BC_losses: 0.006]\n",
      "3.8%┣┫ 114/3.0k [02:47<01:10:55, 1s/it, Loss: 0.396, PDE_losses: 0.114, BC_losses: 0.006]\n",
      "3.9%┣┫ 117/3.0k [02:48<01:09:34, 1s/it, Loss: 0.395, PDE_losses: 0.114, BC_losses: 0.006]\n",
      "4.0%┣┫ 120/3.0k [02:49<01:08:15, 1s/it, Loss: 0.394, PDE_losses: 0.113, BC_losses: 0.006]\n",
      "4.1%┣┫ 123/3.0k [02:50<01:06:59, 1s/it, Loss: 0.393, PDE_losses: 0.112, BC_losses: 0.006]\n",
      "4.2%┣┫ 126/3.0k [02:52<01:05:48, 1s/it, Loss: 0.392, PDE_losses: 0.111, BC_losses: 0.006]\n",
      "4.3%┣┫ 129/3.0k [02:53<01:04:40, 1s/it, Loss: 0.391, PDE_losses: 0.111, BC_losses: 0.006]\n",
      "4.4%┣┫ 132/3.0k [02:54<01:03:38, 1s/it, Loss: 0.391, PDE_losses: 0.110, BC_losses: 0.006]\n",
      "4.5%┣┫ 135/3.0k [02:56<01:02:35, 1s/it, Loss: 0.390, PDE_losses: 0.109, BC_losses: 0.006]\n",
      "4.6%┣┫ 138/3.0k [02:57<01:01:34, 1s/it, Loss: 0.389, PDE_losses: 0.109, BC_losses: 0.006]\n",
      "4.7%┣┫ 141/3.0k [02:58<01:00:39, 1s/it, Loss: 0.388, PDE_losses: 0.108, BC_losses: 0.006]\n",
      "4.8%┣┫ 144/3.0k [03:00<59:46, 1s/it, Loss: 0.387, PDE_losses: 0.107, BC_losses: 0.006]\n",
      "4.9%┣┫ 147/3.0k [03:01<58:55, 1s/it, Loss: 0.387, PDE_losses: 0.107, BC_losses: 0.006]\n",
      "5.0%┣┫ 150/3.0k [03:02<58:05, 1s/it, Loss: 0.386, PDE_losses: 0.107, BC_losses: 0.006]\n",
      "5.1%┣┫ 152/3.0k [03:03<57:36, 1s/it, Loss: 0.386, PDE_losses: 0.106, BC_losses: 0.006]\n",
      "5.2%┣┫ 155/3.0k [03:05<56:51, 1s/it, Loss: 0.384, PDE_losses: 0.105, BC_losses: 0.006]\n",
      "5.3%┣┫ 158/3.0k [03:06<56:08, 1s/it, Loss: 0.384, PDE_losses: 0.104, BC_losses: 0.006]\n",
      "5.3%┣┫ 160/3.0k [03:07<55:41, 1s/it, Loss: 0.384, PDE_losses: 0.105, BC_losses: 0.006]\n",
      "5.4%┣┫ 163/3.0k [03:08<54:59, 1s/it, Loss: 0.384, PDE_losses: 0.104, BC_losses: 0.006]\n",
      "5.5%┣┫ 166/3.0k [03:10<54:18, 1s/it, Loss: 0.383, PDE_losses: 0.104, BC_losses: 0.006]\n",
      "5.6%┣┫ 169/3.0k [03:11<53:37, 1s/it, Loss: 0.382, PDE_losses: 0.103, BC_losses: 0.006]\n",
      "5.7%┣┫ 172/3.0k [03:12<52:59, 1s/it, Loss: 0.381, PDE_losses: 0.103, BC_losses: 0.006]\n",
      "5.8%┣┫ 175/3.0k [03:14<52:23, 1s/it, Loss: 0.381, PDE_losses: 0.102, BC_losses: 0.006]\n",
      "5.9%┣┫ 178/3.0k [03:15<51:45, 1s/it, Loss: 0.379, PDE_losses: 0.102, BC_losses: 0.006]\n",
      "6.0%┣┫ 181/3.0k [03:16<51:12, 1s/it, Loss: 0.379, PDE_losses: 0.102, BC_losses: 0.006]\n",
      "6.1%┣┫ 184/3.0k [03:18<50:39, 1s/it, Loss: 0.379, PDE_losses: 0.101, BC_losses: 0.005]\n",
      "6.2%┣┫ 187/3.0k [03:19<50:06, 1s/it, Loss: 0.379, PDE_losses: 0.101, BC_losses: 0.005]\n",
      "6.3%┣┫ 190/3.0k [03:20<49:35, 1s/it, Loss: 0.378, PDE_losses: 0.100, BC_losses: 0.005]\n",
      "6.4%┣┫ 193/3.0k [03:21<49:04, 1s/it, Loss: 0.377, PDE_losses: 0.100, BC_losses: 0.005]\n",
      "6.5%┣┫ 196/3.0k [03:23<48:34, 1s/it, Loss: 0.377, PDE_losses: 0.100, BC_losses: 0.005]\n",
      "6.6%┣┫ 199/3.0k [03:24<48:06, 1s/it, Loss: 0.376, PDE_losses: 0.099, BC_losses: 0.005]\n",
      "6.7%┣┫ 202/3.0k [03:25<47:37, 1s/it, Loss: 0.376, PDE_losses: 0.099, BC_losses: 0.005]\n",
      "6.8%┣┫ 205/3.0k [03:27<47:09, 1s/it, Loss: 0.375, PDE_losses: 0.099, BC_losses: 0.005]\n",
      "6.9%┣┫ 208/3.0k [03:28<46:42, 1s/it, Loss: 0.375, PDE_losses: 0.099, BC_losses: 0.005]\n",
      "7.0%┣┫ 211/3.0k [03:29<46:16, 1it/s, Loss: 0.374, PDE_losses: 0.098, BC_losses: 0.005]\n",
      "7.1%┣┫ 214/3.0k [03:30<45:51, 1it/s, Loss: 0.374, PDE_losses: 0.098, BC_losses: 0.005]\n",
      "7.2%┣┫ 217/3.0k [03:32<45:27, 1it/s, Loss: 0.373, PDE_losses: 0.098, BC_losses: 0.005]\n",
      "7.3%┣┫ 220/3.0k [03:33<45:03, 1it/s, Loss: 0.372, PDE_losses: 0.097, BC_losses: 0.005]\n",
      "7.4%┣┫ 223/3.0k [03:34<44:40, 1it/s, Loss: 0.372, PDE_losses: 0.097, BC_losses: 0.005]\n",
      "7.5%┣┫ 226/3.0k [03:36<44:19, 1it/s, Loss: 0.372, PDE_losses: 0.097, BC_losses: 0.005]\n",
      "7.6%┣┫ 229/3.0k [03:37<44:00, 1it/s, Loss: 0.371, PDE_losses: 0.096, BC_losses: 0.005]\n",
      "7.7%┣┫ 232/3.0k [03:38<43:38, 1it/s, Loss: 0.371, PDE_losses: 0.097, BC_losses: 0.005]\n",
      "7.8%┣┫ 235/3.0k [03:40<43:16, 1it/s, Loss: 0.371, PDE_losses: 0.096, BC_losses: 0.005]\n",
      "7.9%┣┫ 238/3.0k [03:41<42:55, 1it/s, Loss: 0.370, PDE_losses: 0.096, BC_losses: 0.005]\n",
      "8.0%┣┫ 241/3.0k [03:42<42:35, 1it/s, Loss: 0.369, PDE_losses: 0.096, BC_losses: 0.005]\n",
      "8.1%┣┫ 244/3.0k [03:43<42:14, 1it/s, Loss: 0.369, PDE_losses: 0.095, BC_losses: 0.005]\n",
      "8.2%┣┫ 247/3.0k [03:45<41:54, 1it/s, Loss: 0.369, PDE_losses: 0.095, BC_losses: 0.005]\n",
      "8.3%┣┫ 250/3.0k [03:46<41:34, 1it/s, Loss: 0.368, PDE_losses: 0.095, BC_losses: 0.005]\n",
      "8.4%┣┫ 253/3.0k [03:47<41:14, 1it/s, Loss: 0.368, PDE_losses: 0.095, BC_losses: 0.005]\n",
      "8.5%┣┫ 256/3.0k [03:48<40:57, 1it/s, Loss: 0.368, PDE_losses: 0.094, BC_losses: 0.005]\n",
      "8.6%┣┫ 259/3.0k [03:50<40:40, 1it/s, Loss: 0.367, PDE_losses: 0.094, BC_losses: 0.005]\n",
      "8.7%┣┫ 262/3.0k [03:51<40:22, 1it/s, Loss: 0.367, PDE_losses: 0.094, BC_losses: 0.005]\n",
      "8.8%┣┫ 265/3.0k [03:52<40:05, 1it/s, Loss: 0.366, PDE_losses: 0.094, BC_losses: 0.005]\n",
      "8.9%┣┫ 268/3.0k [03:54<39:50, 1it/s, Loss: 0.366, PDE_losses: 0.094, BC_losses: 0.005]\n",
      "9.0%┣┫ 270/3.0k [03:55<39:41, 1it/s, Loss: 0.365, PDE_losses: 0.093, BC_losses: 0.005]\n",
      "9.1%┣┫ 273/3.0k [03:56<39:25, 1it/s, Loss: 0.364, PDE_losses: 0.093, BC_losses: 0.005]\n",
      "9.2%┣┫ 276/3.0k [03:57<39:09, 1it/s, Loss: 0.364, PDE_losses: 0.094, BC_losses: 0.005]\n",
      "9.3%┣┫ 278/3.0k [03:58<39:00, 1it/s, Loss: 0.364, PDE_losses: 0.093, BC_losses: 0.005]\n",
      "9.4%┣┫ 281/3.0k [04:00<38:46, 1it/s, Loss: 0.363, PDE_losses: 0.093, BC_losses: 0.005]\n",
      "9.5%┣┫ 284/3.0k [04:01<38:30, 1it/s, Loss: 0.363, PDE_losses: 0.092, BC_losses: 0.005]\n",
      "9.6%┣┫ 287/3.0k [04:02<38:16, 1it/s, Loss: 0.362, PDE_losses: 0.092, BC_losses: 0.005]\n",
      "9.7%┣┫ 290/3.0k [04:04<38:04, 1it/s, Loss: 0.362, PDE_losses: 0.092, BC_losses: 0.005]\n",
      "9.8%┣┫ 293/3.0k [04:05<37:52, 1it/s, Loss: 0.362, PDE_losses: 0.092, BC_losses: 0.005]\n",
      "9.9%┣┫ 296/3.0k [04:07<37:40, 1it/s, Loss: 0.361, PDE_losses: 0.092, BC_losses: 0.005]\n",
      "10.0%┣┫ 299/3.0k [04:08<37:28, 1it/s, Loss: 0.361, PDE_losses: 0.092, BC_losses: 0.005]\n",
      "10.1%┣┫ 302/3.0k [04:09<37:15, 1it/s, Loss: 0.360, PDE_losses: 0.092, BC_losses: 0.005]\n",
      "10.2%┣┫ 305/3.0k [04:11<37:02, 1it/s, Loss: 0.360, PDE_losses: 0.091, BC_losses: 0.005]\n",
      "10.3%┣┫ 308/3.0k [04:12<36:49, 1it/s, Loss: 0.359, PDE_losses: 0.091, BC_losses: 0.005]\n",
      "10.4%┣┫ 311/3.0k [04:13<36:36, 1it/s, Loss: 0.359, PDE_losses: 0.091, BC_losses: 0.005]\n",
      "10.5%┣┫ 314/3.0k [04:15<36:24, 1it/s, Loss: 0.358, PDE_losses: 0.091, BC_losses: 0.005]\n",
      "10.5%┣┫ 316/3.0k [04:16<36:18, 1it/s, Loss: 0.358, PDE_losses: 0.090, BC_losses: 0.005]\n",
      "10.6%┣┫ 319/3.0k [04:17<36:06, 1it/s, Loss: 0.358, PDE_losses: 0.091, BC_losses: 0.005]\n",
      "10.7%┣┫ 322/3.0k [04:18<35:54, 1it/s, Loss: 0.357, PDE_losses: 0.091, BC_losses: 0.005]\n",
      "10.8%┣┫ 325/3.0k [04:19<35:41, 1it/s, Loss: 0.356, PDE_losses: 0.090, BC_losses: 0.005]\n",
      "10.9%┣┫ 328/3.0k [04:21<35:30, 1it/s, Loss: 0.356, PDE_losses: 0.090, BC_losses: 0.005]\n",
      "11.0%┣┫ 331/3.0k [04:22<35:18, 1it/s, Loss: 0.356, PDE_losses: 0.090, BC_losses: 0.005]\n",
      "11.1%┣┫ 334/3.0k [04:23<35:07, 1it/s, Loss: 0.355, PDE_losses: 0.089, BC_losses: 0.005]\n",
      "11.2%┣┫ 337/3.0k [04:25<34:57, 1it/s, Loss: 0.354, PDE_losses: 0.090, BC_losses: 0.005]\n",
      "11.3%┣┫ 340/3.0k [04:26<34:46, 1it/s, Loss: 0.354, PDE_losses: 0.090, BC_losses: 0.005]\n",
      "11.4%┣┫ 343/3.0k [04:27<34:36, 1it/s, Loss: 0.353, PDE_losses: 0.090, BC_losses: 0.005]\n",
      "11.5%┣┫ 346/3.0k [04:29<34:27, 1it/s, Loss: 0.352, PDE_losses: 0.089, BC_losses: 0.005]\n",
      "11.6%┣┫ 349/3.0k [04:30<34:16, 1it/s, Loss: 0.352, PDE_losses: 0.089, BC_losses: 0.005]\n",
      "11.7%┣┫ 352/3.0k [04:31<34:05, 1it/s, Loss: 0.352, PDE_losses: 0.089, BC_losses: 0.005]\n",
      "11.8%┣┫ 355/3.0k [04:32<33:55, 1it/s, Loss: 0.351, PDE_losses: 0.089, BC_losses: 0.005]\n",
      "11.9%┣┫ 358/3.0k [04:34<33:46, 1it/s, Loss: 0.351, PDE_losses: 0.088, BC_losses: 0.005]\n",
      "12.0%┣┫ 361/3.0k [04:35<33:37, 1it/s, Loss: 0.350, PDE_losses: 0.088, BC_losses: 0.005]\n",
      "12.1%┣┫ 364/3.0k [04:36<33:28, 1it/s, Loss: 0.349, PDE_losses: 0.088, BC_losses: 0.005]\n",
      "12.2%┣┫ 367/3.0k [04:38<33:19, 1it/s, Loss: 0.349, PDE_losses: 0.088, BC_losses: 0.005]\n",
      "12.3%┣┫ 370/3.0k [04:39<33:09, 1it/s, Loss: 0.348, PDE_losses: 0.088, BC_losses: 0.005]\n",
      "12.4%┣┫ 373/3.0k [04:40<33:01, 1it/s, Loss: 0.347, PDE_losses: 0.087, BC_losses: 0.005]\n",
      "12.5%┣┫ 376/3.0k [04:42<32:51, 1it/s, Loss: 0.347, PDE_losses: 0.088, BC_losses: 0.005]\n",
      "12.6%┣┫ 379/3.0k [04:43<32:42, 1it/s, Loss: 0.346, PDE_losses: 0.088, BC_losses: 0.005]\n",
      "12.7%┣┫ 382/3.0k [04:44<32:33, 1it/s, Loss: 0.345, PDE_losses: 0.087, BC_losses: 0.005]\n",
      "12.8%┣┫ 385/3.0k [04:45<32:24, 1it/s, Loss: 0.345, PDE_losses: 0.087, BC_losses: 0.005]\n",
      "12.9%┣┫ 388/3.0k [04:47<32:15, 1it/s, Loss: 0.345, PDE_losses: 0.087, BC_losses: 0.005]\n",
      "13.0%┣┫ 391/3.0k [04:48<32:07, 1it/s, Loss: 0.344, PDE_losses: 0.086, BC_losses: 0.005]\n",
      "13.1%┣┫ 394/3.0k [04:49<31:59, 1it/s, Loss: 0.343, PDE_losses: 0.086, BC_losses: 0.005]\n",
      "13.2%┣┫ 397/3.0k [04:51<31:51, 1it/s, Loss: 0.342, PDE_losses: 0.086, BC_losses: 0.005]\n",
      "13.3%┣┫ 400/3.0k [04:52<31:42, 1it/s, Loss: 0.341, PDE_losses: 0.085, BC_losses: 0.004]\n",
      "13.4%┣┫ 403/3.0k [04:53<31:34, 1it/s, Loss: 0.340, PDE_losses: 0.085, BC_losses: 0.004]\n",
      "13.5%┣┫ 406/3.0k [04:55<31:27, 1it/s, Loss: 0.340, PDE_losses: 0.085, BC_losses: 0.004]\n",
      "13.6%┣┫ 409/3.0k [04:56<31:20, 1it/s, Loss: 0.339, PDE_losses: 0.085, BC_losses: 0.004]\n",
      "13.7%┣┫ 411/3.0k [04:58<31:20, 1it/s, Loss: 0.339, PDE_losses: 0.085, BC_losses: 0.004]\n",
      "13.8%┣┫ 414/3.0k [04:59<31:11, 1it/s, Loss: 0.338, PDE_losses: 0.085, BC_losses: 0.004]\n",
      "13.9%┣┫ 417/3.0k [05:00<31:05, 1it/s, Loss: 0.337, PDE_losses: 0.084, BC_losses: 0.004]\n",
      "14.0%┣┫ 420/3.0k [05:02<30:57, 1it/s, Loss: 0.336, PDE_losses: 0.084, BC_losses: 0.004]\n",
      "14.1%┣┫ 423/3.0k [05:03<30:49, 1it/s, Loss: 0.335, PDE_losses: 0.084, BC_losses: 0.004]\n",
      "14.2%┣┫ 426/3.0k [05:04<30:41, 1it/s, Loss: 0.334, PDE_losses: 0.084, BC_losses: 0.004]\n",
      "14.3%┣┫ 429/3.0k [05:05<30:33, 1it/s, Loss: 0.333, PDE_losses: 0.084, BC_losses: 0.004]\n",
      "14.4%┣┫ 432/3.0k [05:06<30:26, 1it/s, Loss: 0.332, PDE_losses: 0.083, BC_losses: 0.004]\n",
      "14.5%┣┫ 435/3.0k [05:08<30:18, 1it/s, Loss: 0.332, PDE_losses: 0.083, BC_losses: 0.004]\n",
      "14.6%┣┫ 438/3.0k [05:09<30:10, 1it/s, Loss: 0.331, PDE_losses: 0.083, BC_losses: 0.004]\n",
      "14.7%┣┫ 441/3.0k [05:10<30:03, 1it/s, Loss: 0.329, PDE_losses: 0.083, BC_losses: 0.004]\n",
      "14.8%┣┫ 444/3.0k [05:11<29:55, 1it/s, Loss: 0.329, PDE_losses: 0.083, BC_losses: 0.004]\n",
      "14.9%┣┫ 447/3.0k [05:13<29:49, 1it/s, Loss: 0.328, PDE_losses: 0.082, BC_losses: 0.004]\n",
      "15.0%┣┫ 450/3.0k [05:14<29:42, 1it/s, Loss: 0.328, PDE_losses: 0.083, BC_losses: 0.004]\n",
      "15.1%┣┫ 453/3.0k [05:15<29:36, 1it/s, Loss: 0.326, PDE_losses: 0.082, BC_losses: 0.004]\n",
      "15.2%┣┫ 456/3.0k [05:16<29:29, 1it/s, Loss: 0.325, PDE_losses: 0.082, BC_losses: 0.004]\n",
      "15.3%┣┫ 459/3.0k [05:18<29:22, 1it/s, Loss: 0.324, PDE_losses: 0.082, BC_losses: 0.004]\n",
      "15.4%┣┫ 462/3.0k [05:19<29:15, 1it/s, Loss: 0.323, PDE_losses: 0.081, BC_losses: 0.004]\n",
      "15.5%┣┫ 465/3.0k [05:20<29:09, 1it/s, Loss: 0.322, PDE_losses: 0.081, BC_losses: 0.004]\n",
      "15.6%┣┫ 468/3.0k [05:21<29:03, 1it/s, Loss: 0.321, PDE_losses: 0.080, BC_losses: 0.004]\n",
      "15.7%┣┫ 471/3.0k [05:23<28:56, 1it/s, Loss: 0.320, PDE_losses: 0.080, BC_losses: 0.004]\n",
      "15.8%┣┫ 474/3.0k [05:24<28:49, 1it/s, Loss: 0.319, PDE_losses: 0.080, BC_losses: 0.004]\n",
      "15.9%┣┫ 477/3.0k [05:25<28:43, 1it/s, Loss: 0.317, PDE_losses: 0.080, BC_losses: 0.004]\n",
      "16.0%┣┫ 480/3.0k [05:26<28:37, 1it/s, Loss: 0.316, PDE_losses: 0.079, BC_losses: 0.004]\n",
      "16.1%┣┫ 483/3.0k [05:28<28:31, 1it/s, Loss: 0.315, PDE_losses: 0.079, BC_losses: 0.004]\n",
      "16.2%┣┫ 486/3.0k [05:29<28:26, 1it/s, Loss: 0.313, PDE_losses: 0.079, BC_losses: 0.004]\n",
      "16.3%┣┫ 489/3.0k [05:30<28:19, 1it/s, Loss: 0.312, PDE_losses: 0.079, BC_losses: 0.004]\n",
      "16.4%┣┫ 492/3.0k [05:31<28:13, 1it/s, Loss: 0.311, PDE_losses: 0.078, BC_losses: 0.004]\n",
      "16.5%┣┫ 495/3.0k [05:33<28:07, 1it/s, Loss: 0.310, PDE_losses: 0.078, BC_losses: 0.004]\n",
      "16.6%┣┫ 498/3.0k [05:34<28:01, 1it/s, Loss: 0.308, PDE_losses: 0.077, BC_losses: 0.004]\n",
      "16.7%┣┫ 501/3.0k [05:35<27:55, 1it/s, Loss: 0.307, PDE_losses: 0.078, BC_losses: 0.004]\n",
      "16.8%┣┫ 504/3.0k [05:36<27:49, 1it/s, Loss: 0.305, PDE_losses: 0.077, BC_losses: 0.004]\n",
      "16.9%┣┫ 507/3.0k [05:38<27:45, 1it/s, Loss: 0.304, PDE_losses: 0.076, BC_losses: 0.004]\n",
      "17.0%┣┫ 510/3.0k [05:39<27:39, 2it/s, Loss: 0.302, PDE_losses: 0.076, BC_losses: 0.004]\n",
      "17.1%┣┫ 513/3.0k [05:40<27:33, 2it/s, Loss: 0.301, PDE_losses: 0.076, BC_losses: 0.004]\n",
      "17.2%┣┫ 516/3.0k [05:42<27:29, 2it/s, Loss: 0.300, PDE_losses: 0.076, BC_losses: 0.004]\n",
      "17.3%┣┫ 519/3.0k [05:43<27:23, 2it/s, Loss: 0.298, PDE_losses: 0.075, BC_losses: 0.004]\n",
      "17.4%┣┫ 522/3.0k [05:44<27:17, 2it/s, Loss: 0.296, PDE_losses: 0.075, BC_losses: 0.004]\n",
      "17.5%┣┫ 525/3.0k [05:46<27:13, 2it/s, Loss: 0.294, PDE_losses: 0.075, BC_losses: 0.004]\n",
      "17.6%┣┫ 528/3.0k [05:47<27:08, 2it/s, Loss: 0.292, PDE_losses: 0.074, BC_losses: 0.004]\n",
      "17.7%┣┫ 531/3.0k [05:48<27:03, 2it/s, Loss: 0.291, PDE_losses: 0.074, BC_losses: 0.004]\n",
      "17.8%┣┫ 534/3.0k [05:50<26:59, 2it/s, Loss: 0.290, PDE_losses: 0.073, BC_losses: 0.004]\n",
      "17.9%┣┫ 537/3.0k [05:51<26:54, 2it/s, Loss: 0.288, PDE_losses: 0.072, BC_losses: 0.004]\n",
      "18.0%┣┫ 540/3.0k [05:53<26:50, 2it/s, Loss: 0.285, PDE_losses: 0.073, BC_losses: 0.004]\n",
      "18.1%┣┫ 543/3.0k [05:54<26:45, 2it/s, Loss: 0.284, PDE_losses: 0.073, BC_losses: 0.003]\n",
      "18.2%┣┫ 546/3.0k [05:55<26:40, 2it/s, Loss: 0.282, PDE_losses: 0.071, BC_losses: 0.003]\n",
      "18.3%┣┫ 549/3.0k [05:57<26:35, 2it/s, Loss: 0.280, PDE_losses: 0.071, BC_losses: 0.003]\n",
      "18.4%┣┫ 552/3.0k [05:58<26:29, 2it/s, Loss: 0.278, PDE_losses: 0.072, BC_losses: 0.003]\n",
      "18.5%┣┫ 555/3.0k [05:59<26:24, 2it/s, Loss: 0.276, PDE_losses: 0.070, BC_losses: 0.003]\n",
      "18.6%┣┫ 558/3.0k [06:00<26:19, 2it/s, Loss: 0.274, PDE_losses: 0.071, BC_losses: 0.003]\n",
      "18.7%┣┫ 561/3.0k [06:01<26:14, 2it/s, Loss: 0.272, PDE_losses: 0.070, BC_losses: 0.003]\n",
      "18.8%┣┫ 564/3.0k [06:03<26:09, 2it/s, Loss: 0.270, PDE_losses: 0.070, BC_losses: 0.003]\n",
      "18.9%┣┫ 567/3.0k [06:04<26:04, 2it/s, Loss: 0.268, PDE_losses: 0.069, BC_losses: 0.003]\n",
      "19.0%┣┫ 570/3.0k [06:05<25:58, 2it/s, Loss: 0.266, PDE_losses: 0.069, BC_losses: 0.003]\n",
      "19.1%┣┫ 573/3.0k [06:06<25:54, 2it/s, Loss: 0.264, PDE_losses: 0.069, BC_losses: 0.003]\n",
      "19.2%┣┫ 576/3.0k [06:07<25:49, 2it/s, Loss: 0.262, PDE_losses: 0.068, BC_losses: 0.003]\n",
      "19.3%┣┫ 579/3.0k [06:09<25:44, 2it/s, Loss: 0.260, PDE_losses: 0.069, BC_losses: 0.003]\n",
      "19.4%┣┫ 582/3.0k [06:10<25:40, 2it/s, Loss: 0.258, PDE_losses: 0.068, BC_losses: 0.003]\n",
      "19.5%┣┫ 585/3.0k [06:11<25:36, 2it/s, Loss: 0.256, PDE_losses: 0.068, BC_losses: 0.003]\n",
      "19.6%┣┫ 588/3.0k [06:13<25:31, 2it/s, Loss: 0.253, PDE_losses: 0.067, BC_losses: 0.003]\n",
      "19.7%┣┫ 591/3.0k [06:14<25:27, 2it/s, Loss: 0.251, PDE_losses: 0.067, BC_losses: 0.003]\n",
      "19.8%┣┫ 594/3.0k [06:15<25:23, 2it/s, Loss: 0.250, PDE_losses: 0.066, BC_losses: 0.003]\n",
      "19.9%┣┫ 597/3.0k [06:17<25:18, 2it/s, Loss: 0.247, PDE_losses: 0.066, BC_losses: 0.003]\n",
      "20.0%┣┫ 600/3.0k [06:18<25:16, 2it/s, Loss: 0.245, PDE_losses: 0.066, BC_losses: 0.003]\n",
      "20.1%┣┫ 603/3.0k [06:20<25:12, 2it/s, Loss: 0.243, PDE_losses: 0.066, BC_losses: 0.003]\n",
      "20.2%┣┫ 606/3.0k [06:21<25:08, 2it/s, Loss: 0.241, PDE_losses: 0.065, BC_losses: 0.003]\n",
      "20.3%┣┫ 609/3.0k [06:22<25:03, 2it/s, Loss: 0.238, PDE_losses: 0.065, BC_losses: 0.003]\n",
      "20.4%┣┫ 612/3.0k [06:23<24:59, 2it/s, Loss: 0.236, PDE_losses: 0.065, BC_losses: 0.003]\n",
      "20.5%┣┫ 615/3.0k [06:25<24:54, 2it/s, Loss: 0.234, PDE_losses: 0.065, BC_losses: 0.003]\n",
      "20.6%┣┫ 618/3.0k [06:26<24:50, 2it/s, Loss: 0.231, PDE_losses: 0.064, BC_losses: 0.003]\n",
      "20.7%┣┫ 621/3.0k [06:27<24:46, 2it/s, Loss: 0.229, PDE_losses: 0.064, BC_losses: 0.003]\n",
      "20.8%┣┫ 624/3.0k [06:28<24:41, 2it/s, Loss: 0.226, PDE_losses: 0.064, BC_losses: 0.003]\n",
      "20.9%┣┫ 627/3.0k [06:30<24:37, 2it/s, Loss: 0.224, PDE_losses: 0.064, BC_losses: 0.003]\n",
      "21.0%┣┫ 630/3.0k [06:31<24:32, 2it/s, Loss: 0.222, PDE_losses: 0.063, BC_losses: 0.003]\n",
      "21.1%┣┫ 633/3.0k [06:32<24:29, 2it/s, Loss: 0.219, PDE_losses: 0.064, BC_losses: 0.003]\n",
      "21.2%┣┫ 636/3.0k [06:34<24:25, 2it/s, Loss: 0.219, PDE_losses: 0.064, BC_losses: 0.003]\n",
      "21.3%┣┫ 639/3.0k [06:35<24:21, 2it/s, Loss: 0.215, PDE_losses: 0.063, BC_losses: 0.003]\n",
      "21.4%┣┫ 642/3.0k [06:36<24:16, 2it/s, Loss: 0.214, PDE_losses: 0.064, BC_losses: 0.003]\n",
      "21.5%┣┫ 645/3.0k [06:37<24:12, 2it/s, Loss: 0.211, PDE_losses: 0.063, BC_losses: 0.003]\n",
      "21.6%┣┫ 648/3.0k [06:38<24:08, 2it/s, Loss: 0.208, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "21.7%┣┫ 651/3.0k [06:40<24:04, 2it/s, Loss: 0.206, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "21.8%┣┫ 654/3.0k [06:41<24:00, 2it/s, Loss: 0.203, PDE_losses: 0.062, BC_losses: 0.002]\n",
      "21.9%┣┫ 657/3.0k [06:42<23:55, 2it/s, Loss: 0.201, PDE_losses: 0.062, BC_losses: 0.002]\n",
      "22.0%┣┫ 660/3.0k [06:43<23:51, 2it/s, Loss: 0.199, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "22.1%┣┫ 663/3.0k [06:44<23:47, 2it/s, Loss: 0.198, PDE_losses: 0.062, BC_losses: 0.002]\n",
      "22.2%┣┫ 666/3.0k [06:45<23:42, 2it/s, Loss: 0.195, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "22.3%┣┫ 669/3.0k [06:47<23:39, 2it/s, Loss: 0.194, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "22.4%┣┫ 672/3.0k [06:48<23:35, 2it/s, Loss: 0.192, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "22.5%┣┫ 675/3.0k [06:49<23:31, 2it/s, Loss: 0.189, PDE_losses: 0.062, BC_losses: 0.002]\n",
      "22.6%┣┫ 678/3.0k [06:50<23:27, 2it/s, Loss: 0.187, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "22.7%┣┫ 681/3.0k [06:52<23:24, 2it/s, Loss: 0.185, PDE_losses: 0.062, BC_losses: 0.002]\n",
      "22.8%┣┫ 684/3.0k [06:53<23:21, 2it/s, Loss: 0.183, PDE_losses: 0.062, BC_losses: 0.002]\n",
      "22.9%┣┫ 687/3.0k [06:54<23:17, 2it/s, Loss: 0.181, PDE_losses: 0.062, BC_losses: 0.002]\n",
      "23.0%┣┫ 690/3.0k [06:56<23:13, 2it/s, Loss: 0.180, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "23.1%┣┫ 693/3.0k [06:57<23:10, 2it/s, Loss: 0.177, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "23.2%┣┫ 696/3.0k [06:58<23:06, 2it/s, Loss: 0.176, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "23.3%┣┫ 698/3.0k [06:59<23:04, 2it/s, Loss: 0.173, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "23.3%┣┫ 700/3.0k [07:00<23:02, 2it/s, Loss: 0.173, PDE_losses: 0.062, BC_losses: 0.002]\n",
      "23.4%┣┫ 703/3.0k [07:01<22:59, 2it/s, Loss: 0.171, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "23.5%┣┫ 706/3.0k [07:03<22:55, 2it/s, Loss: 0.170, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "23.6%┣┫ 709/3.0k [07:04<22:52, 2it/s, Loss: 0.168, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "23.7%┣┫ 712/3.0k [07:05<22:48, 2it/s, Loss: 0.167, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "23.8%┣┫ 715/3.0k [07:06<22:45, 2it/s, Loss: 0.163, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "23.9%┣┫ 718/3.0k [07:08<22:41, 2it/s, Loss: 0.163, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "24.0%┣┫ 721/3.0k [07:09<22:37, 2it/s, Loss: 0.160, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "24.1%┣┫ 724/3.0k [07:10<22:34, 2it/s, Loss: 0.160, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "24.2%┣┫ 726/3.0k [07:11<22:32, 2it/s, Loss: 0.158, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "24.3%┣┫ 729/3.0k [07:12<22:29, 2it/s, Loss: 0.157, PDE_losses: 0.061, BC_losses: 0.002]\n",
      "24.4%┣┫ 732/3.0k [07:14<22:26, 2it/s, Loss: 0.156, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "24.5%┣┫ 735/3.0k [07:15<22:22, 2it/s, Loss: 0.155, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "24.6%┣┫ 738/3.0k [07:16<22:19, 2it/s, Loss: 0.153, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "24.7%┣┫ 741/3.0k [07:17<22:15, 2it/s, Loss: 0.151, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "24.8%┣┫ 744/3.0k [07:19<22:12, 2it/s, Loss: 0.149, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "24.9%┣┫ 747/3.0k [07:20<22:08, 2it/s, Loss: 0.149, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "25.0%┣┫ 750/3.0k [07:21<22:05, 2it/s, Loss: 0.147, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "25.1%┣┫ 753/3.0k [07:22<22:02, 2it/s, Loss: 0.146, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "25.2%┣┫ 756/3.0k [07:24<21:58, 2it/s, Loss: 0.146, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "25.3%┣┫ 759/3.0k [07:25<21:55, 2it/s, Loss: 0.144, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "25.4%┣┫ 762/3.0k [07:26<21:51, 2it/s, Loss: 0.145, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "25.5%┣┫ 765/3.0k [07:27<21:48, 2it/s, Loss: 0.143, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "25.6%┣┫ 768/3.0k [07:28<21:45, 2it/s, Loss: 0.142, PDE_losses: 0.067, BC_losses: 0.002]\n",
      "25.7%┣┫ 771/3.0k [07:30<21:41, 2it/s, Loss: 0.140, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "25.8%┣┫ 774/3.0k [07:31<21:38, 2it/s, Loss: 0.139, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "25.9%┣┫ 777/3.0k [07:32<21:35, 2it/s, Loss: 0.137, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "26.0%┣┫ 780/3.0k [07:33<21:32, 2it/s, Loss: 0.137, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "26.1%┣┫ 783/3.0k [07:35<21:29, 2it/s, Loss: 0.135, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "26.2%┣┫ 786/3.0k [07:36<21:26, 2it/s, Loss: 0.134, PDE_losses: 0.068, BC_losses: 0.002]\n",
      "26.3%┣┫ 789/3.0k [07:37<21:23, 2it/s, Loss: 0.135, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "26.4%┣┫ 792/3.0k [07:38<21:20, 2it/s, Loss: 0.133, PDE_losses: 0.067, BC_losses: 0.002]\n",
      "26.5%┣┫ 795/3.0k [07:39<21:16, 2it/s, Loss: 0.132, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "26.6%┣┫ 798/3.0k [07:41<21:13, 2it/s, Loss: 0.134, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "26.7%┣┫ 801/3.0k [07:42<21:09, 2it/s, Loss: 0.130, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "26.8%┣┫ 804/3.0k [07:43<21:06, 2it/s, Loss: 0.127, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "26.9%┣┫ 807/3.0k [07:44<21:03, 2it/s, Loss: 0.129, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "27.0%┣┫ 810/3.0k [07:46<21:00, 2it/s, Loss: 0.127, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "27.1%┣┫ 813/3.0k [07:47<20:57, 2it/s, Loss: 0.129, PDE_losses: 0.067, BC_losses: 0.002]\n",
      "27.2%┣┫ 816/3.0k [07:48<20:55, 2it/s, Loss: 0.127, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "27.3%┣┫ 819/3.0k [07:49<20:52, 2it/s, Loss: 0.124, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "27.4%┣┫ 822/3.0k [07:51<20:48, 2it/s, Loss: 0.124, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "27.5%┣┫ 825/3.0k [07:52<20:45, 2it/s, Loss: 0.124, PDE_losses: 0.067, BC_losses: 0.002]\n",
      "27.6%┣┫ 828/3.0k [07:53<20:42, 2it/s, Loss: 0.123, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "27.7%┣┫ 831/3.0k [07:54<20:39, 2it/s, Loss: 0.124, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "27.8%┣┫ 834/3.0k [07:56<20:37, 2it/s, Loss: 0.123, PDE_losses: 0.068, BC_losses: 0.002]\n",
      "27.9%┣┫ 837/3.0k [07:57<20:34, 2it/s, Loss: 0.120, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "28.0%┣┫ 840/3.0k [07:58<20:32, 2it/s, Loss: 0.122, PDE_losses: 0.067, BC_losses: 0.002]\n",
      "28.1%┣┫ 843/3.0k [08:00<20:29, 2it/s, Loss: 0.121, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "28.2%┣┫ 846/3.0k [08:01<20:26, 2it/s, Loss: 0.121, PDE_losses: 0.067, BC_losses: 0.002]\n",
      "28.3%┣┫ 849/3.0k [08:02<20:23, 2it/s, Loss: 0.118, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "28.4%┣┫ 852/3.0k [08:04<20:20, 2it/s, Loss: 0.116, PDE_losses: 0.068, BC_losses: 0.002]\n",
      "28.5%┣┫ 855/3.0k [08:05<20:18, 2it/s, Loss: 0.116, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "28.6%┣┫ 858/3.0k [08:06<20:14, 2it/s, Loss: 0.116, PDE_losses: 0.067, BC_losses: 0.002]\n",
      "28.7%┣┫ 861/3.0k [08:07<20:12, 2it/s, Loss: 0.116, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "28.8%┣┫ 864/3.0k [08:08<20:09, 2it/s, Loss: 0.116, PDE_losses: 0.067, BC_losses: 0.002]\n",
      "28.9%┣┫ 867/3.0k [08:10<20:06, 2it/s, Loss: 0.117, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "29.0%┣┫ 869/3.0k [08:11<20:06, 2it/s, Loss: 0.115, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "29.1%┣┫ 872/3.0k [08:12<20:03, 2it/s, Loss: 0.114, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "29.2%┣┫ 875/3.0k [08:14<20:00, 2it/s, Loss: 0.115, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "29.3%┣┫ 878/3.0k [08:15<19:57, 2it/s, Loss: 0.113, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "29.4%┣┫ 881/3.0k [08:16<19:54, 2it/s, Loss: 0.113, PDE_losses: 0.067, BC_losses: 0.002]\n",
      "29.5%┣┫ 884/3.0k [08:17<19:52, 2it/s, Loss: 0.110, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "29.6%┣┫ 887/3.0k [08:19<19:49, 2it/s, Loss: 0.111, PDE_losses: 0.068, BC_losses: 0.002]\n",
      "29.7%┣┫ 890/3.0k [08:20<19:47, 2it/s, Loss: 0.110, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "29.8%┣┫ 893/3.0k [08:21<19:44, 2it/s, Loss: 0.110, PDE_losses: 0.067, BC_losses: 0.002]\n",
      "29.9%┣┫ 896/3.0k [08:23<19:42, 2it/s, Loss: 0.109, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "30.0%┣┫ 899/3.0k [08:24<19:39, 2it/s, Loss: 0.108, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "30.1%┣┫ 902/3.0k [08:25<19:36, 2it/s, Loss: 0.108, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "30.2%┣┫ 905/3.0k [08:26<19:33, 2it/s, Loss: 0.107, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "30.3%┣┫ 908/3.0k [08:27<19:31, 2it/s, Loss: 0.107, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "30.4%┣┫ 911/3.0k [08:29<19:28, 2it/s, Loss: 0.105, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "30.5%┣┫ 914/3.0k [08:30<19:25, 2it/s, Loss: 0.107, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "30.6%┣┫ 917/3.0k [08:31<19:22, 2it/s, Loss: 0.105, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "30.7%┣┫ 920/3.0k [08:32<19:20, 2it/s, Loss: 0.103, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "30.8%┣┫ 923/3.0k [08:34<19:17, 2it/s, Loss: 0.105, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "30.9%┣┫ 926/3.0k [08:35<19:14, 2it/s, Loss: 0.105, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "31.0%┣┫ 929/3.0k [08:36<19:12, 2it/s, Loss: 0.108, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "31.1%┣┫ 932/3.0k [08:38<19:10, 2it/s, Loss: 0.101, PDE_losses: 0.067, BC_losses: 0.002]\n",
      "31.1%┣┫ 934/3.0k [08:39<19:08, 2it/s, Loss: 0.104, PDE_losses: 0.067, BC_losses: 0.002]\n",
      "31.2%┣┫ 937/3.0k [08:40<19:06, 2it/s, Loss: 0.103, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "31.3%┣┫ 940/3.0k [08:41<19:04, 2it/s, Loss: 0.103, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "31.4%┣┫ 943/3.0k [08:43<19:01, 2it/s, Loss: 0.101, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "31.5%┣┫ 946/3.0k [08:44<18:59, 2it/s, Loss: 0.102, PDE_losses: 0.062, BC_losses: 0.002]\n",
      "31.6%┣┫ 949/3.0k [08:45<18:56, 2it/s, Loss: 0.102, PDE_losses: 0.064, BC_losses: 0.002]\n",
      "31.7%┣┫ 952/3.0k [08:46<18:53, 2it/s, Loss: 0.100, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "31.8%┣┫ 955/3.0k [08:47<18:50, 2it/s, Loss: 0.100, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "31.9%┣┫ 958/3.0k [08:48<18:48, 2it/s, Loss: 0.098, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "32.0%┣┫ 961/3.0k [08:50<18:45, 2it/s, Loss: 0.100, PDE_losses: 0.066, BC_losses: 0.002]\n",
      "32.1%┣┫ 964/3.0k [08:51<18:43, 2it/s, Loss: 0.099, PDE_losses: 0.062, BC_losses: 0.002]\n",
      "32.2%┣┫ 967/3.0k [08:52<18:40, 2it/s, Loss: 0.096, PDE_losses: 0.063, BC_losses: 0.002]\n",
      "32.3%┣┫ 970/3.0k [08:54<18:38, 2it/s, Loss: 0.098, PDE_losses: 0.062, BC_losses: 0.002]\n",
      "32.4%┣┫ 973/3.0k [08:55<18:35, 2it/s, Loss: 0.099, PDE_losses: 0.065, BC_losses: 0.002]\n",
      "32.5%┣┫ 976/3.0k [08:56<18:33, 2it/s, Loss: 0.097, PDE_losses: 0.065, BC_losses: 0.001]\n",
      "32.6%┣┫ 979/3.0k [08:57<18:30, 2it/s, Loss: 0.098, PDE_losses: 0.062, BC_losses: 0.001]\n",
      "32.7%┣┫ 982/3.0k [08:58<18:27, 2it/s, Loss: 0.095, PDE_losses: 0.063, BC_losses: 0.001]\n",
      "32.8%┣┫ 985/3.0k [09:00<18:25, 2it/s, Loss: 0.095, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "32.9%┣┫ 988/3.0k [09:01<18:23, 2it/s, Loss: 0.096, PDE_losses: 0.063, BC_losses: 0.001]\n",
      "33.0%┣┫ 991/3.0k [09:02<18:20, 2it/s, Loss: 0.098, PDE_losses: 0.063, BC_losses: 0.001]\n",
      "33.1%┣┫ 994/3.0k [09:04<18:18, 2it/s, Loss: 0.094, PDE_losses: 0.064, BC_losses: 0.001]\n",
      "33.2%┣┫ 997/3.0k [09:05<18:16, 2it/s, Loss: 0.094, PDE_losses: 0.063, BC_losses: 0.001]\n",
      "33.3%┣┫ 1.0k/3.0k [09:06<18:13, 2it/s, Loss: 0.095, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "33.4%┣┫ 1.0k/3.0k [09:07<18:11, 2it/s, Loss: 0.094, PDE_losses: 0.062, BC_losses: 0.001]\n",
      "33.5%┣┫ 1.0k/3.0k [09:08<18:08, 2it/s, Loss: 0.093, PDE_losses: 0.064, BC_losses: 0.001]\n",
      "33.6%┣┫ 1.0k/3.0k [09:10<18:06, 2it/s, Loss: 0.093, PDE_losses: 0.064, BC_losses: 0.001]\n",
      "33.7%┣┫ 1.0k/3.0k [09:11<18:03, 2it/s, Loss: 0.092, PDE_losses: 0.067, BC_losses: 0.001]\n",
      "33.8%┣┫ 1.0k/3.0k [09:12<18:01, 2it/s, Loss: 0.099, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "33.9%┣┫ 1.0k/3.0k [09:13<17:59, 2it/s, Loss: 0.092, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "34.0%┣┫ 1.0k/3.0k [09:15<17:56, 2it/s, Loss: 0.091, PDE_losses: 0.062, BC_losses: 0.001]\n",
      "34.1%┣┫ 1.0k/3.0k [09:16<17:54, 2it/s, Loss: 0.091, PDE_losses: 0.065, BC_losses: 0.001]\n",
      "34.2%┣┫ 1.0k/3.0k [09:17<17:52, 2it/s, Loss: 0.091, PDE_losses: 0.062, BC_losses: 0.001]\n",
      "34.3%┣┫ 1.0k/3.0k [09:19<17:49, 2it/s, Loss: 0.092, PDE_losses: 0.063, BC_losses: 0.001]\n",
      "34.4%┣┫ 1.0k/3.0k [09:20<17:47, 2it/s, Loss: 0.095, PDE_losses: 0.063, BC_losses: 0.001]\n",
      "34.5%┣┫ 1.0k/3.0k [09:21<17:45, 2it/s, Loss: 0.089, PDE_losses: 0.064, BC_losses: 0.001]\n",
      "34.6%┣┫ 1.0k/3.0k [09:23<17:43, 2it/s, Loss: 0.088, PDE_losses: 0.063, BC_losses: 0.001]\n",
      "34.7%┣┫ 1.0k/3.0k [09:24<17:40, 2it/s, Loss: 0.091, PDE_losses: 0.062, BC_losses: 0.001]\n",
      "34.8%┣┫ 1.0k/3.0k [09:25<17:38, 2it/s, Loss: 0.095, PDE_losses: 0.064, BC_losses: 0.001]\n",
      "34.9%┣┫ 1.0k/3.0k [09:26<17:36, 2it/s, Loss: 0.089, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "35.0%┣┫ 1.1k/3.0k [09:27<17:33, 2it/s, Loss: 0.087, PDE_losses: 0.063, BC_losses: 0.001]\n",
      "35.1%┣┫ 1.1k/3.0k [09:29<17:31, 2it/s, Loss: 0.092, PDE_losses: 0.064, BC_losses: 0.001]\n",
      "35.2%┣┫ 1.1k/3.0k [09:30<17:29, 2it/s, Loss: 0.090, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "35.3%┣┫ 1.1k/3.0k [09:31<17:26, 2it/s, Loss: 0.092, PDE_losses: 0.064, BC_losses: 0.001]\n",
      "35.4%┣┫ 1.1k/3.0k [09:32<17:24, 2it/s, Loss: 0.088, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "35.5%┣┫ 1.1k/3.0k [09:34<17:22, 2it/s, Loss: 0.089, PDE_losses: 0.062, BC_losses: 0.001]\n",
      "35.6%┣┫ 1.1k/3.0k [09:35<17:19, 2it/s, Loss: 0.092, PDE_losses: 0.064, BC_losses: 0.001]\n",
      "35.7%┣┫ 1.1k/3.0k [09:36<17:17, 2it/s, Loss: 0.089, PDE_losses: 0.062, BC_losses: 0.001]\n",
      "35.8%┣┫ 1.1k/3.0k [09:38<17:17, 2it/s, Loss: 0.089, PDE_losses: 0.064, BC_losses: 0.001]\n",
      "35.9%┣┫ 1.1k/3.0k [09:39<17:15, 2it/s, Loss: 0.087, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "36.0%┣┫ 1.1k/3.0k [09:40<17:14, 2it/s, Loss: 0.086, PDE_losses: 0.063, BC_losses: 0.001]\n",
      "36.1%┣┫ 1.1k/3.0k [09:41<17:11, 2it/s, Loss: 0.085, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "36.2%┣┫ 1.1k/3.0k [09:43<17:09, 2it/s, Loss: 0.091, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "36.3%┣┫ 1.1k/3.0k [09:44<17:07, 2it/s, Loss: 0.085, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "36.4%┣┫ 1.1k/3.0k [09:45<17:05, 2it/s, Loss: 0.089, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "36.5%┣┫ 1.1k/3.0k [09:47<17:03, 2it/s, Loss: 0.089, PDE_losses: 0.063, BC_losses: 0.001]\n",
      "36.6%┣┫ 1.1k/3.0k [09:48<17:00, 2it/s, Loss: 0.089, PDE_losses: 0.062, BC_losses: 0.001]\n",
      "36.7%┣┫ 1.1k/3.0k [09:49<16:58, 2it/s, Loss: 0.085, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "36.8%┣┫ 1.1k/3.0k [09:50<16:56, 2it/s, Loss: 0.085, PDE_losses: 0.064, BC_losses: 0.001]\n",
      "36.9%┣┫ 1.1k/3.0k [09:52<16:54, 2it/s, Loss: 0.087, PDE_losses: 0.062, BC_losses: 0.001]\n",
      "37.0%┣┫ 1.1k/3.0k [09:53<16:52, 2it/s, Loss: 0.085, PDE_losses: 0.064, BC_losses: 0.001]\n",
      "37.1%┣┫ 1.1k/3.0k [09:54<16:50, 2it/s, Loss: 0.087, PDE_losses: 0.062, BC_losses: 0.001]\n",
      "37.2%┣┫ 1.1k/3.0k [09:55<16:47, 2it/s, Loss: 0.087, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "37.3%┣┫ 1.1k/3.0k [09:56<16:45, 2it/s, Loss: 0.084, PDE_losses: 0.064, BC_losses: 0.001]\n",
      "37.4%┣┫ 1.1k/3.0k [09:58<16:43, 2it/s, Loss: 0.083, PDE_losses: 0.062, BC_losses: 0.001]\n",
      "37.5%┣┫ 1.1k/3.0k [09:59<16:40, 2it/s, Loss: 0.084, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "37.6%┣┫ 1.1k/3.0k [10:00<16:38, 2it/s, Loss: 0.083, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "37.7%┣┫ 1.1k/3.0k [10:01<16:36, 2it/s, Loss: 0.082, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "37.8%┣┫ 1.1k/3.0k [10:03<16:34, 2it/s, Loss: 0.085, PDE_losses: 0.063, BC_losses: 0.001]\n",
      "37.9%┣┫ 1.1k/3.0k [10:04<16:32, 2it/s, Loss: 0.088, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "38.0%┣┫ 1.1k/3.0k [10:05<16:30, 2it/s, Loss: 0.081, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "38.1%┣┫ 1.1k/3.0k [10:06<16:28, 2it/s, Loss: 0.085, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "38.2%┣┫ 1.1k/3.0k [10:08<16:26, 2it/s, Loss: 0.085, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "38.3%┣┫ 1.1k/3.0k [10:09<16:24, 2it/s, Loss: 0.084, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "38.4%┣┫ 1.2k/3.0k [10:11<16:22, 2it/s, Loss: 0.081, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "38.5%┣┫ 1.2k/3.0k [10:12<16:20, 2it/s, Loss: 0.083, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "38.6%┣┫ 1.2k/3.0k [10:13<16:17, 2it/s, Loss: 0.085, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "38.7%┣┫ 1.2k/3.0k [10:14<16:15, 2it/s, Loss: 0.083, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "38.8%┣┫ 1.2k/3.0k [10:16<16:13, 2it/s, Loss: 0.085, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "38.9%┣┫ 1.2k/3.0k [10:17<16:11, 2it/s, Loss: 0.082, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "39.0%┣┫ 1.2k/3.0k [10:18<16:09, 2it/s, Loss: 0.083, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "39.1%┣┫ 1.2k/3.0k [10:19<16:07, 2it/s, Loss: 0.084, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "39.2%┣┫ 1.2k/3.0k [10:20<16:04, 2it/s, Loss: 0.082, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "39.3%┣┫ 1.2k/3.0k [10:22<16:02, 2it/s, Loss: 0.083, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "39.4%┣┫ 1.2k/3.0k [10:23<16:00, 2it/s, Loss: 0.081, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "39.5%┣┫ 1.2k/3.0k [10:24<15:58, 2it/s, Loss: 0.082, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "39.6%┣┫ 1.2k/3.0k [10:25<15:56, 2it/s, Loss: 0.080, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "39.7%┣┫ 1.2k/3.0k [10:26<15:54, 2it/s, Loss: 0.082, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "39.8%┣┫ 1.2k/3.0k [10:28<15:51, 2it/s, Loss: 0.080, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "39.9%┣┫ 1.2k/3.0k [10:29<15:49, 2it/s, Loss: 0.081, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "40.0%┣┫ 1.2k/3.0k [10:30<15:47, 2it/s, Loss: 0.083, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "40.1%┣┫ 1.2k/3.0k [10:31<15:45, 2it/s, Loss: 0.078, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "40.2%┣┫ 1.2k/3.0k [10:33<15:43, 2it/s, Loss: 0.080, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "40.3%┣┫ 1.2k/3.0k [10:34<15:41, 2it/s, Loss: 0.084, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "40.4%┣┫ 1.2k/3.0k [10:35<15:39, 2it/s, Loss: 0.080, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "40.5%┣┫ 1.2k/3.0k [10:36<15:37, 2it/s, Loss: 0.081, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "40.6%┣┫ 1.2k/3.0k [10:38<15:35, 2it/s, Loss: 0.078, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "40.7%┣┫ 1.2k/3.0k [10:39<15:33, 2it/s, Loss: 0.079, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "40.8%┣┫ 1.2k/3.0k [10:40<15:30, 2it/s, Loss: 0.081, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "40.9%┣┫ 1.2k/3.0k [10:41<15:28, 2it/s, Loss: 0.077, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "41.0%┣┫ 1.2k/3.0k [10:42<15:26, 2it/s, Loss: 0.075, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "41.1%┣┫ 1.2k/3.0k [10:43<15:24, 2it/s, Loss: 0.077, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "41.2%┣┫ 1.2k/3.0k [10:45<15:22, 2it/s, Loss: 0.076, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "41.3%┣┫ 1.2k/3.0k [10:46<15:20, 2it/s, Loss: 0.078, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "41.4%┣┫ 1.2k/3.0k [10:47<15:18, 2it/s, Loss: 0.082, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "41.5%┣┫ 1.2k/3.0k [10:48<15:16, 2it/s, Loss: 0.078, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "41.6%┣┫ 1.2k/3.0k [10:50<15:14, 2it/s, Loss: 0.081, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "41.7%┣┫ 1.2k/3.0k [10:51<15:12, 2it/s, Loss: 0.078, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "41.8%┣┫ 1.3k/3.0k [10:52<15:10, 2it/s, Loss: 0.080, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "41.9%┣┫ 1.3k/3.0k [10:53<15:08, 2it/s, Loss: 0.075, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "42.0%┣┫ 1.3k/3.0k [10:55<15:06, 2it/s, Loss: 0.081, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "42.1%┣┫ 1.3k/3.0k [10:56<15:04, 2it/s, Loss: 0.077, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "42.2%┣┫ 1.3k/3.0k [10:57<15:02, 2it/s, Loss: 0.074, PDE_losses: 0.061, BC_losses: 0.001]\n",
      "42.3%┣┫ 1.3k/3.0k [10:58<15:00, 2it/s, Loss: 0.078, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "42.4%┣┫ 1.3k/3.0k [10:59<14:58, 2it/s, Loss: 0.077, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "42.5%┣┫ 1.3k/3.0k [11:00<14:56, 2it/s, Loss: 0.075, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "42.6%┣┫ 1.3k/3.0k [11:02<14:54, 2it/s, Loss: 0.081, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "42.7%┣┫ 1.3k/3.0k [11:03<14:52, 2it/s, Loss: 0.075, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "42.8%┣┫ 1.3k/3.0k [11:04<14:50, 2it/s, Loss: 0.073, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "42.9%┣┫ 1.3k/3.0k [11:06<14:48, 2it/s, Loss: 0.075, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "43.0%┣┫ 1.3k/3.0k [11:07<14:46, 2it/s, Loss: 0.079, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "43.1%┣┫ 1.3k/3.0k [11:08<14:44, 2it/s, Loss: 0.079, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "43.2%┣┫ 1.3k/3.0k [11:10<14:42, 2it/s, Loss: 0.081, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "43.3%┣┫ 1.3k/3.0k [11:11<14:40, 2it/s, Loss: 0.075, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "43.4%┣┫ 1.3k/3.0k [11:12<14:39, 2it/s, Loss: 0.079, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "43.5%┣┫ 1.3k/3.0k [11:13<14:37, 2it/s, Loss: 0.076, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "43.6%┣┫ 1.3k/3.0k [11:15<14:35, 2it/s, Loss: 0.077, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "43.7%┣┫ 1.3k/3.0k [11:16<14:33, 2it/s, Loss: 0.073, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "43.8%┣┫ 1.3k/3.0k [11:17<14:31, 2it/s, Loss: 0.079, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "43.9%┣┫ 1.3k/3.0k [11:19<14:29, 2it/s, Loss: 0.076, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "44.0%┣┫ 1.3k/3.0k [11:20<14:27, 2it/s, Loss: 0.076, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "44.1%┣┫ 1.3k/3.0k [11:21<14:25, 2it/s, Loss: 0.075, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "44.1%┣┫ 1.3k/3.0k [11:22<14:24, 2it/s, Loss: 0.074, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "44.2%┣┫ 1.3k/3.0k [11:23<14:22, 2it/s, Loss: 0.076, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "44.3%┣┫ 1.3k/3.0k [11:25<14:20, 2it/s, Loss: 0.075, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "44.4%┣┫ 1.3k/3.0k [11:26<14:18, 2it/s, Loss: 0.072, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "44.5%┣┫ 1.3k/3.0k [11:27<14:17, 2it/s, Loss: 0.076, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "44.6%┣┫ 1.3k/3.0k [11:29<14:15, 2it/s, Loss: 0.077, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "44.7%┣┫ 1.3k/3.0k [11:30<14:13, 2it/s, Loss: 0.074, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "44.8%┣┫ 1.3k/3.0k [11:31<14:11, 2it/s, Loss: 0.073, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "44.9%┣┫ 1.3k/3.0k [11:33<14:10, 2it/s, Loss: 0.076, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "45.0%┣┫ 1.4k/3.0k [11:34<14:08, 2it/s, Loss: 0.077, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "45.1%┣┫ 1.4k/3.0k [11:36<14:07, 2it/s, Loss: 0.076, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "45.2%┣┫ 1.4k/3.0k [11:37<14:05, 2it/s, Loss: 0.075, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "45.3%┣┫ 1.4k/3.0k [11:38<14:04, 2it/s, Loss: 0.073, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "45.4%┣┫ 1.4k/3.0k [11:39<14:02, 2it/s, Loss: 0.075, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "45.5%┣┫ 1.4k/3.0k [11:41<14:00, 2it/s, Loss: 0.075, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "45.6%┣┫ 1.4k/3.0k [11:42<13:58, 2it/s, Loss: 0.074, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "45.7%┣┫ 1.4k/3.0k [11:43<13:56, 2it/s, Loss: 0.073, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "45.8%┣┫ 1.4k/3.0k [11:44<13:54, 2it/s, Loss: 0.073, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "45.9%┣┫ 1.4k/3.0k [11:46<13:52, 2it/s, Loss: 0.075, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "46.0%┣┫ 1.4k/3.0k [11:47<13:51, 2it/s, Loss: 0.075, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "46.1%┣┫ 1.4k/3.0k [11:48<13:49, 2it/s, Loss: 0.073, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "46.2%┣┫ 1.4k/3.0k [11:49<13:47, 2it/s, Loss: 0.073, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "46.3%┣┫ 1.4k/3.0k [11:51<13:45, 2it/s, Loss: 0.069, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "46.4%┣┫ 1.4k/3.0k [11:52<13:43, 2it/s, Loss: 0.074, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "46.5%┣┫ 1.4k/3.0k [11:54<13:42, 2it/s, Loss: 0.077, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "46.6%┣┫ 1.4k/3.0k [11:55<13:40, 2it/s, Loss: 0.070, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "46.7%┣┫ 1.4k/3.0k [11:56<13:38, 2it/s, Loss: 0.072, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "46.8%┣┫ 1.4k/3.0k [11:57<13:36, 2it/s, Loss: 0.077, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "46.9%┣┫ 1.4k/3.0k [11:58<13:34, 2it/s, Loss: 0.074, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "47.0%┣┫ 1.4k/3.0k [12:00<13:32, 2it/s, Loss: 0.071, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "47.1%┣┫ 1.4k/3.0k [12:01<13:30, 2it/s, Loss: 0.073, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "47.2%┣┫ 1.4k/3.0k [12:02<13:29, 2it/s, Loss: 0.070, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "47.3%┣┫ 1.4k/3.0k [12:04<13:27, 2it/s, Loss: 0.073, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "47.4%┣┫ 1.4k/3.0k [12:05<13:25, 2it/s, Loss: 0.070, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "47.5%┣┫ 1.4k/3.0k [12:06<13:23, 2it/s, Loss: 0.068, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "47.6%┣┫ 1.4k/3.0k [12:08<13:22, 2it/s, Loss: 0.072, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "47.7%┣┫ 1.4k/3.0k [12:09<13:20, 2it/s, Loss: 0.073, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "47.8%┣┫ 1.4k/3.0k [12:10<13:18, 2it/s, Loss: 0.071, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "47.9%┣┫ 1.4k/3.0k [12:11<13:16, 2it/s, Loss: 0.072, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "48.0%┣┫ 1.4k/3.0k [12:13<13:14, 2it/s, Loss: 0.070, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "48.1%┣┫ 1.4k/3.0k [12:14<13:12, 2it/s, Loss: 0.073, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "48.2%┣┫ 1.4k/3.0k [12:15<13:10, 2it/s, Loss: 0.071, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "48.3%┣┫ 1.4k/3.0k [12:16<13:08, 2it/s, Loss: 0.070, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "48.4%┣┫ 1.5k/3.0k [12:17<13:07, 2it/s, Loss: 0.073, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "48.5%┣┫ 1.5k/3.0k [12:19<13:05, 2it/s, Loss: 0.072, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "48.6%┣┫ 1.5k/3.0k [12:20<13:03, 2it/s, Loss: 0.069, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "48.7%┣┫ 1.5k/3.0k [12:21<13:01, 2it/s, Loss: 0.073, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "48.8%┣┫ 1.5k/3.0k [12:23<13:00, 2it/s, Loss: 0.072, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "48.9%┣┫ 1.5k/3.0k [12:24<12:58, 2it/s, Loss: 0.072, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "49.0%┣┫ 1.5k/3.0k [12:25<12:56, 2it/s, Loss: 0.072, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "49.1%┣┫ 1.5k/3.0k [12:26<12:54, 2it/s, Loss: 0.073, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "49.2%┣┫ 1.5k/3.0k [12:28<12:52, 2it/s, Loss: 0.073, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "49.3%┣┫ 1.5k/3.0k [12:29<12:51, 2it/s, Loss: 0.072, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "49.4%┣┫ 1.5k/3.0k [12:30<12:49, 2it/s, Loss: 0.072, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "49.5%┣┫ 1.5k/3.0k [12:32<12:47, 2it/s, Loss: 0.071, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "49.6%┣┫ 1.5k/3.0k [12:33<12:46, 2it/s, Loss: 0.072, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "49.7%┣┫ 1.5k/3.0k [12:34<12:44, 2it/s, Loss: 0.069, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "49.8%┣┫ 1.5k/3.0k [12:35<12:42, 2it/s, Loss: 0.072, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "49.9%┣┫ 1.5k/3.0k [12:37<12:40, 2it/s, Loss: 0.071, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "50.0%┣┫ 1.5k/3.0k [12:38<12:39, 2it/s, Loss: 0.070, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "50.1%┣┫ 1.5k/3.0k [12:39<12:37, 2it/s, Loss: 0.070, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "50.2%┣┫ 1.5k/3.0k [12:41<12:35, 2it/s, Loss: 0.072, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "50.3%┣┫ 1.5k/3.0k [12:42<12:33, 2it/s, Loss: 0.065, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "50.4%┣┫ 1.5k/3.0k [12:43<12:32, 2it/s, Loss: 0.074, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "50.5%┣┫ 1.5k/3.0k [12:45<12:30, 2it/s, Loss: 0.073, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "50.6%┣┫ 1.5k/3.0k [12:46<12:28, 2it/s, Loss: 0.072, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "50.7%┣┫ 1.5k/3.0k [12:47<12:26, 2it/s, Loss: 0.072, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "50.8%┣┫ 1.5k/3.0k [12:48<12:25, 2it/s, Loss: 0.073, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "50.9%┣┫ 1.5k/3.0k [12:50<12:23, 2it/s, Loss: 0.070, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "51.0%┣┫ 1.5k/3.0k [12:51<12:21, 2it/s, Loss: 0.069, PDE_losses: 0.060, BC_losses: 0.001]\n",
      "51.1%┣┫ 1.5k/3.0k [12:52<12:19, 2it/s, Loss: 0.071, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "51.2%┣┫ 1.5k/3.0k [12:53<12:18, 2it/s, Loss: 0.071, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "51.3%┣┫ 1.5k/3.0k [12:55<12:16, 2it/s, Loss: 0.067, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "51.4%┣┫ 1.5k/3.0k [12:56<12:14, 2it/s, Loss: 0.072, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "51.5%┣┫ 1.5k/3.0k [12:57<12:12, 2it/s, Loss: 0.069, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "51.6%┣┫ 1.5k/3.0k [12:58<12:11, 2it/s, Loss: 0.073, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "51.7%┣┫ 1.6k/3.0k [13:00<12:09, 2it/s, Loss: 0.068, PDE_losses: 0.059, BC_losses: 0.001]\n",
      "51.8%┣┫ 1.6k/3.0k [13:01<12:07, 2it/s, Loss: 0.071, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "51.9%┣┫ 1.6k/3.0k [13:02<12:05, 2it/s, Loss: 0.070, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "52.0%┣┫ 1.6k/3.0k [13:03<12:03, 2it/s, Loss: 0.066, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "52.1%┣┫ 1.6k/3.0k [13:05<12:02, 2it/s, Loss: 0.071, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "52.2%┣┫ 1.6k/3.0k [13:06<12:00, 2it/s, Loss: 0.070, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "52.3%┣┫ 1.6k/3.0k [13:07<11:58, 2it/s, Loss: 0.069, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "52.4%┣┫ 1.6k/3.0k [13:08<11:57, 2it/s, Loss: 0.070, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "52.5%┣┫ 1.6k/3.0k [13:10<11:55, 2it/s, Loss: 0.068, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "52.6%┣┫ 1.6k/3.0k [13:11<11:53, 2it/s, Loss: 0.071, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "52.7%┣┫ 1.6k/3.0k [13:12<11:52, 2it/s, Loss: 0.074, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "52.8%┣┫ 1.6k/3.0k [13:14<11:51, 2it/s, Loss: 0.069, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "52.9%┣┫ 1.6k/3.0k [13:16<11:49, 2it/s, Loss: 0.067, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "53.0%┣┫ 1.6k/3.0k [13:17<11:48, 2it/s, Loss: 0.071, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "53.1%┣┫ 1.6k/3.0k [13:18<11:46, 2it/s, Loss: 0.067, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "53.2%┣┫ 1.6k/3.0k [13:19<11:44, 2it/s, Loss: 0.067, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "53.3%┣┫ 1.6k/3.0k [13:20<11:43, 2it/s, Loss: 0.070, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "53.4%┣┫ 1.6k/3.0k [13:22<11:41, 2it/s, Loss: 0.066, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "53.5%┣┫ 1.6k/3.0k [13:23<11:39, 2it/s, Loss: 0.070, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "53.6%┣┫ 1.6k/3.0k [13:24<11:38, 2it/s, Loss: 0.068, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "53.7%┣┫ 1.6k/3.0k [13:26<11:36, 2it/s, Loss: 0.069, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "53.8%┣┫ 1.6k/3.0k [13:27<11:34, 2it/s, Loss: 0.069, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "53.9%┣┫ 1.6k/3.0k [13:28<11:33, 2it/s, Loss: 0.070, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "53.9%┣┫ 1.6k/3.0k [13:29<11:32, 2it/s, Loss: 0.064, PDE_losses: 0.056, BC_losses: 0.001]\n",
      "54.0%┣┫ 1.6k/3.0k [13:31<11:30, 2it/s, Loss: 0.066, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "54.1%┣┫ 1.6k/3.0k [13:32<11:28, 2it/s, Loss: 0.073, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "54.2%┣┫ 1.6k/3.0k [13:33<11:27, 2it/s, Loss: 0.067, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "54.3%┣┫ 1.6k/3.0k [13:34<11:25, 2it/s, Loss: 0.070, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "54.4%┣┫ 1.6k/3.0k [13:35<11:23, 2it/s, Loss: 0.073, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "54.5%┣┫ 1.6k/3.0k [13:37<11:21, 2it/s, Loss: 0.065, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "54.6%┣┫ 1.6k/3.0k [13:38<11:20, 2it/s, Loss: 0.068, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "54.7%┣┫ 1.6k/3.0k [13:39<11:18, 2it/s, Loss: 0.070, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "54.8%┣┫ 1.6k/3.0k [13:41<11:16, 2it/s, Loss: 0.070, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "54.9%┣┫ 1.6k/3.0k [13:42<11:15, 2it/s, Loss: 0.065, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "55.0%┣┫ 1.7k/3.0k [13:43<11:13, 2it/s, Loss: 0.066, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "55.1%┣┫ 1.7k/3.0k [13:44<11:11, 2it/s, Loss: 0.068, PDE_losses: 0.058, BC_losses: 0.001]\n",
      "55.2%┣┫ 1.7k/3.0k [13:46<11:10, 2it/s, Loss: 0.067, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "55.3%┣┫ 1.7k/3.0k [13:47<11:08, 2it/s, Loss: 0.066, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "55.4%┣┫ 1.7k/3.0k [13:48<11:06, 2it/s, Loss: 0.066, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "55.5%┣┫ 1.7k/3.0k [13:49<11:04, 2it/s, Loss: 0.062, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "55.6%┣┫ 1.7k/3.0k [13:51<11:03, 2it/s, Loss: 0.068, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "55.7%┣┫ 1.7k/3.0k [13:52<11:01, 2it/s, Loss: 0.069, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "55.8%┣┫ 1.7k/3.0k [13:53<11:00, 2it/s, Loss: 0.065, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "55.9%┣┫ 1.7k/3.0k [13:54<10:58, 2it/s, Loss: 0.068, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "56.0%┣┫ 1.7k/3.0k [13:56<10:56, 2it/s, Loss: 0.063, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "56.1%┣┫ 1.7k/3.0k [13:57<10:55, 2it/s, Loss: 0.066, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "56.2%┣┫ 1.7k/3.0k [13:58<10:53, 2it/s, Loss: 0.068, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "56.3%┣┫ 1.7k/3.0k [13:59<10:51, 2it/s, Loss: 0.071, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "56.4%┣┫ 1.7k/3.0k [14:01<10:49, 2it/s, Loss: 0.066, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "56.5%┣┫ 1.7k/3.0k [14:02<10:48, 2it/s, Loss: 0.068, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "56.6%┣┫ 1.7k/3.0k [14:03<10:46, 2it/s, Loss: 0.067, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "56.7%┣┫ 1.7k/3.0k [14:04<10:44, 2it/s, Loss: 0.065, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "56.8%┣┫ 1.7k/3.0k [14:05<10:42, 2it/s, Loss: 0.064, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "56.9%┣┫ 1.7k/3.0k [14:07<10:41, 2it/s, Loss: 0.065, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "57.0%┣┫ 1.7k/3.0k [14:08<10:39, 2it/s, Loss: 0.064, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "57.1%┣┫ 1.7k/3.0k [14:09<10:37, 2it/s, Loss: 0.062, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "57.2%┣┫ 1.7k/3.0k [14:10<10:36, 2it/s, Loss: 0.066, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "57.3%┣┫ 1.7k/3.0k [14:12<10:34, 2it/s, Loss: 0.064, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "57.4%┣┫ 1.7k/3.0k [14:13<10:32, 2it/s, Loss: 0.060, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "57.5%┣┫ 1.7k/3.0k [14:14<10:31, 2it/s, Loss: 0.066, PDE_losses: 0.057, BC_losses: 0.001]\n",
      "57.6%┣┫ 1.7k/3.0k [14:16<10:29, 2it/s, Loss: 0.063, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "57.7%┣┫ 1.7k/3.0k [14:17<10:28, 2it/s, Loss: 0.068, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "57.8%┣┫ 1.7k/3.0k [14:18<10:26, 2it/s, Loss: 0.063, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "57.9%┣┫ 1.7k/3.0k [14:20<10:25, 2it/s, Loss: 0.063, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "58.0%┣┫ 1.7k/3.0k [14:21<10:23, 2it/s, Loss: 0.068, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "58.1%┣┫ 1.7k/3.0k [14:22<10:21, 2it/s, Loss: 0.061, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "58.2%┣┫ 1.7k/3.0k [14:23<10:19, 2it/s, Loss: 0.065, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "58.3%┣┫ 1.8k/3.0k [14:24<10:18, 2it/s, Loss: 0.063, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "58.4%┣┫ 1.8k/3.0k [14:26<10:16, 2it/s, Loss: 0.064, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "58.5%┣┫ 1.8k/3.0k [14:27<10:15, 2it/s, Loss: 0.062, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "58.6%┣┫ 1.8k/3.0k [14:28<10:13, 2it/s, Loss: 0.065, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "58.7%┣┫ 1.8k/3.0k [14:29<10:11, 2it/s, Loss: 0.064, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "58.8%┣┫ 1.8k/3.0k [14:31<10:10, 2it/s, Loss: 0.067, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "58.9%┣┫ 1.8k/3.0k [14:32<10:08, 2it/s, Loss: 0.063, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "59.0%┣┫ 1.8k/3.0k [14:33<10:06, 2it/s, Loss: 0.061, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "59.1%┣┫ 1.8k/3.0k [14:35<10:05, 2it/s, Loss: 0.063, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "59.2%┣┫ 1.8k/3.0k [14:36<10:03, 2it/s, Loss: 0.062, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "59.3%┣┫ 1.8k/3.0k [14:37<10:02, 2it/s, Loss: 0.064, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "59.4%┣┫ 1.8k/3.0k [14:38<10:00, 2it/s, Loss: 0.062, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "59.5%┣┫ 1.8k/3.0k [14:40<09:58, 2it/s, Loss: 0.064, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "59.6%┣┫ 1.8k/3.0k [14:41<09:57, 2it/s, Loss: 0.062, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "59.7%┣┫ 1.8k/3.0k [14:42<09:55, 2it/s, Loss: 0.061, PDE_losses: 0.054, BC_losses: 0.001]\n",
      "59.8%┣┫ 1.8k/3.0k [14:43<09:53, 2it/s, Loss: 0.067, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "59.9%┣┫ 1.8k/3.0k [14:45<09:52, 2it/s, Loss: 0.063, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "60.0%┣┫ 1.8k/3.0k [14:46<09:50, 2it/s, Loss: 0.067, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "60.1%┣┫ 1.8k/3.0k [14:47<09:48, 2it/s, Loss: 0.065, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "60.2%┣┫ 1.8k/3.0k [14:48<09:47, 2it/s, Loss: 0.067, PDE_losses: 0.055, BC_losses: 0.001]\n",
      "60.3%┣┫ 1.8k/3.0k [14:50<09:45, 2it/s, Loss: 0.066, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "60.4%┣┫ 1.8k/3.0k [14:51<09:44, 2it/s, Loss: 0.063, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "60.5%┣┫ 1.8k/3.0k [14:52<09:42, 2it/s, Loss: 0.062, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "60.6%┣┫ 1.8k/3.0k [14:54<09:40, 2it/s, Loss: 0.066, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "60.7%┣┫ 1.8k/3.0k [14:55<09:39, 2it/s, Loss: 0.060, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "60.8%┣┫ 1.8k/3.0k [14:56<09:38, 2it/s, Loss: 0.064, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "60.9%┣┫ 1.8k/3.0k [14:57<09:36, 2it/s, Loss: 0.064, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "60.9%┣┫ 1.8k/3.0k [14:58<09:36, 2it/s, Loss: 0.061, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "61.0%┣┫ 1.8k/3.0k [14:59<09:35, 2it/s, Loss: 0.061, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "61.1%┣┫ 1.8k/3.0k [15:00<09:34, 2it/s, Loss: 0.064, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "61.2%┣┫ 1.8k/3.0k [15:02<09:32, 2it/s, Loss: 0.062, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "61.3%┣┫ 1.8k/3.0k [15:03<09:30, 2it/s, Loss: 0.061, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "61.4%┣┫ 1.8k/3.0k [15:04<09:28, 2it/s, Loss: 0.060, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "61.5%┣┫ 1.8k/3.0k [15:05<09:27, 2it/s, Loss: 0.062, PDE_losses: 0.053, BC_losses: 0.001]\n",
      "61.6%┣┫ 1.8k/3.0k [15:06<09:25, 2it/s, Loss: 0.064, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "61.7%┣┫ 1.9k/3.0k [15:08<09:24, 2it/s, Loss: 0.064, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "61.8%┣┫ 1.9k/3.0k [15:09<09:22, 2it/s, Loss: 0.061, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "61.9%┣┫ 1.9k/3.0k [15:10<09:20, 2it/s, Loss: 0.057, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "62.0%┣┫ 1.9k/3.0k [15:11<09:19, 2it/s, Loss: 0.064, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "62.1%┣┫ 1.9k/3.0k [15:12<09:17, 2it/s, Loss: 0.064, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "62.2%┣┫ 1.9k/3.0k [15:14<09:16, 2it/s, Loss: 0.059, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "62.3%┣┫ 1.9k/3.0k [15:15<09:14, 2it/s, Loss: 0.063, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "62.4%┣┫ 1.9k/3.0k [15:16<09:13, 2it/s, Loss: 0.060, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "62.5%┣┫ 1.9k/3.0k [15:18<09:12, 2it/s, Loss: 0.062, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "62.6%┣┫ 1.9k/3.0k [15:19<09:10, 2it/s, Loss: 0.058, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "62.7%┣┫ 1.9k/3.0k [15:20<09:09, 2it/s, Loss: 0.065, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "62.8%┣┫ 1.9k/3.0k [15:22<09:07, 2it/s, Loss: 0.062, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "62.9%┣┫ 1.9k/3.0k [15:23<09:05, 2it/s, Loss: 0.058, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "63.0%┣┫ 1.9k/3.0k [15:24<09:04, 2it/s, Loss: 0.061, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "63.1%┣┫ 1.9k/3.0k [15:25<09:02, 2it/s, Loss: 0.061, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "63.2%┣┫ 1.9k/3.0k [15:27<09:01, 2it/s, Loss: 0.060, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "63.3%┣┫ 1.9k/3.0k [15:28<08:59, 2it/s, Loss: 0.059, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "63.4%┣┫ 1.9k/3.0k [15:29<08:58, 2it/s, Loss: 0.059, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "63.5%┣┫ 1.9k/3.0k [15:31<08:56, 2it/s, Loss: 0.060, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "63.6%┣┫ 1.9k/3.0k [15:32<08:54, 2it/s, Loss: 0.058, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "63.7%┣┫ 1.9k/3.0k [15:33<08:53, 2it/s, Loss: 0.062, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "63.8%┣┫ 1.9k/3.0k [15:34<08:51, 2it/s, Loss: 0.060, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "63.9%┣┫ 1.9k/3.0k [15:36<08:50, 2it/s, Loss: 0.061, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "64.0%┣┫ 1.9k/3.0k [15:37<08:48, 2it/s, Loss: 0.059, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "64.1%┣┫ 1.9k/3.0k [15:38<08:47, 2it/s, Loss: 0.059, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "64.2%┣┫ 1.9k/3.0k [15:40<08:45, 2it/s, Loss: 0.060, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "64.3%┣┫ 1.9k/3.0k [15:41<08:43, 2it/s, Loss: 0.063, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "64.4%┣┫ 1.9k/3.0k [15:42<08:42, 2it/s, Loss: 0.058, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "64.5%┣┫ 1.9k/3.0k [15:43<08:40, 2it/s, Loss: 0.061, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "64.6%┣┫ 1.9k/3.0k [15:45<08:39, 2it/s, Loss: 0.056, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "64.7%┣┫ 1.9k/3.0k [15:46<08:37, 2it/s, Loss: 0.057, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "64.8%┣┫ 1.9k/3.0k [15:47<08:36, 2it/s, Loss: 0.060, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "64.9%┣┫ 1.9k/3.0k [15:49<08:34, 2it/s, Loss: 0.057, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "65.0%┣┫ 1.9k/3.0k [15:50<08:32, 2it/s, Loss: 0.065, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "65.1%┣┫ 2.0k/3.0k [15:51<08:31, 2it/s, Loss: 0.059, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "65.2%┣┫ 2.0k/3.0k [15:52<08:29, 2it/s, Loss: 0.058, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "65.3%┣┫ 2.0k/3.0k [15:54<08:28, 2it/s, Loss: 0.058, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "65.4%┣┫ 2.0k/3.0k [15:55<08:26, 2it/s, Loss: 0.060, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "65.5%┣┫ 2.0k/3.0k [15:56<08:25, 2it/s, Loss: 0.061, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "65.6%┣┫ 2.0k/3.0k [15:58<08:23, 2it/s, Loss: 0.057, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "65.6%┣┫ 2.0k/3.0k [15:59<08:22, 2it/s, Loss: 0.057, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "65.7%┣┫ 2.0k/3.0k [16:00<08:21, 2it/s, Loss: 0.059, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "65.8%┣┫ 2.0k/3.0k [16:01<08:19, 2it/s, Loss: 0.059, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "65.9%┣┫ 2.0k/3.0k [16:03<08:18, 2it/s, Loss: 0.057, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "66.0%┣┫ 2.0k/3.0k [16:04<08:16, 2it/s, Loss: 0.059, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "66.1%┣┫ 2.0k/3.0k [16:05<08:15, 2it/s, Loss: 0.055, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "66.2%┣┫ 2.0k/3.0k [16:06<08:14, 2it/s, Loss: 0.056, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "66.2%┣┫ 2.0k/3.0k [16:07<08:13, 2it/s, Loss: 0.057, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "66.3%┣┫ 2.0k/3.0k [16:09<08:12, 2it/s, Loss: 0.059, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "66.4%┣┫ 2.0k/3.0k [16:10<08:10, 2it/s, Loss: 0.058, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "66.5%┣┫ 2.0k/3.0k [16:11<08:09, 2it/s, Loss: 0.060, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "66.6%┣┫ 2.0k/3.0k [16:13<08:07, 2it/s, Loss: 0.057, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "66.7%┣┫ 2.0k/3.0k [16:14<08:06, 2it/s, Loss: 0.054, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "66.8%┣┫ 2.0k/3.0k [16:15<08:04, 2it/s, Loss: 0.062, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "66.9%┣┫ 2.0k/3.0k [16:16<08:03, 2it/s, Loss: 0.059, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "67.0%┣┫ 2.0k/3.0k [16:18<08:01, 2it/s, Loss: 0.057, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "67.1%┣┫ 2.0k/3.0k [16:19<08:00, 2it/s, Loss: 0.059, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "67.2%┣┫ 2.0k/3.0k [16:21<07:58, 2it/s, Loss: 0.058, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "67.3%┣┫ 2.0k/3.0k [16:22<07:57, 2it/s, Loss: 0.059, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "67.4%┣┫ 2.0k/3.0k [16:23<07:55, 2it/s, Loss: 0.055, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "67.5%┣┫ 2.0k/3.0k [16:25<07:54, 2it/s, Loss: 0.059, PDE_losses: 0.050, BC_losses: 0.001]\n",
      "67.6%┣┫ 2.0k/3.0k [16:26<07:52, 2it/s, Loss: 0.056, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "67.7%┣┫ 2.0k/3.0k [16:27<07:51, 2it/s, Loss: 0.056, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "67.8%┣┫ 2.0k/3.0k [16:29<07:49, 2it/s, Loss: 0.056, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "67.9%┣┫ 2.0k/3.0k [16:30<07:48, 2it/s, Loss: 0.058, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "68.0%┣┫ 2.0k/3.0k [16:31<07:46, 2it/s, Loss: 0.056, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "68.1%┣┫ 2.0k/3.0k [16:33<07:44, 2it/s, Loss: 0.057, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "68.2%┣┫ 2.0k/3.0k [16:34<07:43, 2it/s, Loss: 0.062, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "68.3%┣┫ 2.0k/3.0k [16:35<07:41, 2it/s, Loss: 0.055, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "68.4%┣┫ 2.1k/3.0k [16:36<07:40, 2it/s, Loss: 0.055, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "68.5%┣┫ 2.1k/3.0k [16:38<07:38, 2it/s, Loss: 0.058, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "68.6%┣┫ 2.1k/3.0k [16:39<07:37, 2it/s, Loss: 0.060, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "68.7%┣┫ 2.1k/3.0k [16:40<07:35, 2it/s, Loss: 0.055, PDE_losses: 0.052, BC_losses: 0.001]\n",
      "68.8%┣┫ 2.1k/3.0k [16:42<07:34, 2it/s, Loss: 0.056, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "68.9%┣┫ 2.1k/3.0k [16:43<07:32, 2it/s, Loss: 0.057, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "69.0%┣┫ 2.1k/3.0k [16:44<07:31, 2it/s, Loss: 0.059, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "69.1%┣┫ 2.1k/3.0k [16:46<07:29, 2it/s, Loss: 0.057, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "69.2%┣┫ 2.1k/3.0k [16:47<07:28, 2it/s, Loss: 0.057, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "69.3%┣┫ 2.1k/3.0k [16:48<07:26, 2it/s, Loss: 0.055, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "69.4%┣┫ 2.1k/3.0k [16:50<07:25, 2it/s, Loss: 0.056, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "69.5%┣┫ 2.1k/3.0k [16:51<07:23, 2it/s, Loss: 0.060, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "69.6%┣┫ 2.1k/3.0k [16:52<07:22, 2it/s, Loss: 0.060, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "69.7%┣┫ 2.1k/3.0k [16:53<07:20, 2it/s, Loss: 0.057, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "69.8%┣┫ 2.1k/3.0k [16:55<07:19, 2it/s, Loss: 0.056, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "69.9%┣┫ 2.1k/3.0k [16:56<07:17, 2it/s, Loss: 0.058, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "70.0%┣┫ 2.1k/3.0k [16:58<07:16, 2it/s, Loss: 0.057, PDE_losses: 0.051, BC_losses: 0.001]\n",
      "70.1%┣┫ 2.1k/3.0k [16:59<07:14, 2it/s, Loss: 0.055, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "70.2%┣┫ 2.1k/3.0k [17:00<07:13, 2it/s, Loss: 0.056, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "70.3%┣┫ 2.1k/3.0k [17:01<07:11, 2it/s, Loss: 0.058, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "70.4%┣┫ 2.1k/3.0k [17:03<07:09, 2it/s, Loss: 0.056, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "70.5%┣┫ 2.1k/3.0k [17:04<07:08, 2it/s, Loss: 0.054, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "70.6%┣┫ 2.1k/3.0k [17:05<07:06, 2it/s, Loss: 0.055, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "70.7%┣┫ 2.1k/3.0k [17:06<07:05, 2it/s, Loss: 0.058, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "70.8%┣┫ 2.1k/3.0k [17:08<07:03, 2it/s, Loss: 0.057, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "70.9%┣┫ 2.1k/3.0k [17:09<07:02, 2it/s, Loss: 0.057, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "71.0%┣┫ 2.1k/3.0k [17:10<07:00, 2it/s, Loss: 0.050, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "71.1%┣┫ 2.1k/3.0k [17:11<06:59, 2it/s, Loss: 0.056, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "71.2%┣┫ 2.1k/3.0k [17:13<06:58, 2it/s, Loss: 0.053, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "71.3%┣┫ 2.1k/3.0k [17:14<06:57, 2it/s, Loss: 0.053, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "71.4%┣┫ 2.1k/3.0k [17:15<06:56, 2it/s, Loss: 0.056, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "71.5%┣┫ 2.1k/3.0k [17:17<06:54, 2it/s, Loss: 0.056, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "71.6%┣┫ 2.1k/3.0k [17:18<06:53, 2it/s, Loss: 0.055, PDE_losses: 0.046, BC_losses: 0.001]\n",
      "71.7%┣┫ 2.1k/3.0k [17:20<06:51, 2it/s, Loss: 0.060, PDE_losses: 0.048, BC_losses: 0.001]\n",
      "71.8%┣┫ 2.2k/3.0k [17:21<06:50, 2it/s, Loss: 0.056, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "71.9%┣┫ 2.2k/3.0k [17:22<06:48, 2it/s, Loss: 0.055, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "72.0%┣┫ 2.2k/3.0k [17:23<06:47, 2it/s, Loss: 0.056, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "72.1%┣┫ 2.2k/3.0k [17:24<06:45, 2it/s, Loss: 0.057, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "72.2%┣┫ 2.2k/3.0k [17:26<06:43, 2it/s, Loss: 0.053, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "72.3%┣┫ 2.2k/3.0k [17:27<06:42, 2it/s, Loss: 0.057, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "72.4%┣┫ 2.2k/3.0k [17:28<06:40, 2it/s, Loss: 0.052, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "72.5%┣┫ 2.2k/3.0k [17:30<06:39, 2it/s, Loss: 0.058, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "72.6%┣┫ 2.2k/3.0k [17:31<06:37, 2it/s, Loss: 0.052, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "72.7%┣┫ 2.2k/3.0k [17:32<06:36, 2it/s, Loss: 0.056, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "72.8%┣┫ 2.2k/3.0k [17:33<06:34, 2it/s, Loss: 0.052, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "72.9%┣┫ 2.2k/3.0k [17:35<06:33, 2it/s, Loss: 0.053, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "73.0%┣┫ 2.2k/3.0k [17:36<06:31, 2it/s, Loss: 0.056, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "73.1%┣┫ 2.2k/3.0k [17:37<06:30, 2it/s, Loss: 0.054, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "73.2%┣┫ 2.2k/3.0k [17:39<06:28, 2it/s, Loss: 0.057, PDE_losses: 0.049, BC_losses: 0.001]\n",
      "73.3%┣┫ 2.2k/3.0k [17:40<06:27, 2it/s, Loss: 0.055, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "73.4%┣┫ 2.2k/3.0k [17:41<06:25, 2it/s, Loss: 0.055, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "73.5%┣┫ 2.2k/3.0k [17:42<06:24, 2it/s, Loss: 0.053, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "73.6%┣┫ 2.2k/3.0k [17:44<06:22, 2it/s, Loss: 0.052, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "73.7%┣┫ 2.2k/3.0k [17:45<06:21, 2it/s, Loss: 0.050, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "73.8%┣┫ 2.2k/3.0k [17:46<06:19, 2it/s, Loss: 0.053, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "73.9%┣┫ 2.2k/3.0k [17:48<06:18, 2it/s, Loss: 0.054, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "74.0%┣┫ 2.2k/3.0k [17:49<06:16, 2it/s, Loss: 0.055, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "74.1%┣┫ 2.2k/3.0k [17:50<06:15, 2it/s, Loss: 0.053, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "74.2%┣┫ 2.2k/3.0k [17:51<06:13, 2it/s, Loss: 0.052, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "74.3%┣┫ 2.2k/3.0k [17:52<06:12, 2it/s, Loss: 0.055, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "74.4%┣┫ 2.2k/3.0k [17:53<06:10, 2it/s, Loss: 0.055, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "74.5%┣┫ 2.2k/3.0k [17:55<06:09, 2it/s, Loss: 0.052, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "74.6%┣┫ 2.2k/3.0k [17:56<06:07, 2it/s, Loss: 0.051, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "74.7%┣┫ 2.2k/3.0k [17:57<06:06, 2it/s, Loss: 0.046, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "74.8%┣┫ 2.2k/3.0k [17:58<06:04, 2it/s, Loss: 0.053, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "74.9%┣┫ 2.2k/3.0k [18:00<06:03, 2it/s, Loss: 0.049, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "75.0%┣┫ 2.2k/3.0k [18:01<06:01, 2it/s, Loss: 0.052, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "75.1%┣┫ 2.3k/3.0k [18:02<06:00, 2it/s, Loss: 0.055, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "75.2%┣┫ 2.3k/3.0k [18:03<05:58, 2it/s, Loss: 0.052, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "75.3%┣┫ 2.3k/3.0k [18:05<05:57, 2it/s, Loss: 0.051, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "75.4%┣┫ 2.3k/3.0k [18:06<05:55, 2it/s, Loss: 0.052, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "75.5%┣┫ 2.3k/3.0k [18:07<05:54, 2it/s, Loss: 0.049, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "75.5%┣┫ 2.3k/3.0k [18:08<05:53, 2it/s, Loss: 0.048, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "75.6%┣┫ 2.3k/3.0k [18:10<05:51, 2it/s, Loss: 0.051, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "75.7%┣┫ 2.3k/3.0k [18:11<05:50, 2it/s, Loss: 0.053, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "75.8%┣┫ 2.3k/3.0k [18:12<05:48, 2it/s, Loss: 0.056, PDE_losses: 0.047, BC_losses: 0.001]\n",
      "75.9%┣┫ 2.3k/3.0k [18:13<05:47, 2it/s, Loss: 0.050, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "76.0%┣┫ 2.3k/3.0k [18:15<05:45, 2it/s, Loss: 0.053, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "76.1%┣┫ 2.3k/3.0k [18:16<05:44, 2it/s, Loss: 0.055, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "76.2%┣┫ 2.3k/3.0k [18:17<05:42, 2it/s, Loss: 0.050, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "76.3%┣┫ 2.3k/3.0k [18:18<05:41, 2it/s, Loss: 0.052, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "76.4%┣┫ 2.3k/3.0k [18:19<05:40, 2it/s, Loss: 0.054, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "76.5%┣┫ 2.3k/3.0k [18:21<05:38, 2it/s, Loss: 0.049, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "76.6%┣┫ 2.3k/3.0k [18:22<05:37, 2it/s, Loss: 0.050, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "76.7%┣┫ 2.3k/3.0k [18:23<05:35, 2it/s, Loss: 0.049, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "76.8%┣┫ 2.3k/3.0k [18:25<05:34, 2it/s, Loss: 0.050, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "76.9%┣┫ 2.3k/3.0k [18:26<05:32, 2it/s, Loss: 0.053, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "77.0%┣┫ 2.3k/3.0k [18:27<05:31, 2it/s, Loss: 0.052, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "77.1%┣┫ 2.3k/3.0k [18:28<05:29, 2it/s, Loss: 0.050, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "77.2%┣┫ 2.3k/3.0k [18:30<05:28, 2it/s, Loss: 0.052, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "77.3%┣┫ 2.3k/3.0k [18:31<05:26, 2it/s, Loss: 0.048, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "77.4%┣┫ 2.3k/3.0k [18:32<05:25, 2it/s, Loss: 0.049, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "77.5%┣┫ 2.3k/3.0k [18:34<05:23, 2it/s, Loss: 0.051, PDE_losses: 0.038, BC_losses: 0.001]\n",
      "77.6%┣┫ 2.3k/3.0k [18:35<05:22, 2it/s, Loss: 0.049, PDE_losses: 0.038, BC_losses: 0.001]\n",
      "77.7%┣┫ 2.3k/3.0k [18:36<05:21, 2it/s, Loss: 0.050, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "77.8%┣┫ 2.3k/3.0k [18:38<05:19, 2it/s, Loss: 0.048, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "77.9%┣┫ 2.3k/3.0k [18:39<05:18, 2it/s, Loss: 0.050, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "78.0%┣┫ 2.3k/3.0k [18:41<05:16, 2it/s, Loss: 0.045, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "78.1%┣┫ 2.3k/3.0k [18:42<05:15, 2it/s, Loss: 0.053, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "78.2%┣┫ 2.3k/3.0k [18:43<05:13, 2it/s, Loss: 0.053, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "78.3%┣┫ 2.3k/3.0k [18:44<05:12, 2it/s, Loss: 0.049, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "78.4%┣┫ 2.4k/3.0k [18:46<05:10, 2it/s, Loss: 0.048, PDE_losses: 0.045, BC_losses: 0.001]\n",
      "78.5%┣┫ 2.4k/3.0k [18:47<05:09, 2it/s, Loss: 0.051, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "78.6%┣┫ 2.4k/3.0k [18:48<05:07, 2it/s, Loss: 0.046, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "78.7%┣┫ 2.4k/3.0k [18:49<05:06, 2it/s, Loss: 0.050, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "78.8%┣┫ 2.4k/3.0k [18:51<05:04, 2it/s, Loss: 0.055, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "78.9%┣┫ 2.4k/3.0k [18:52<05:03, 2it/s, Loss: 0.047, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "79.0%┣┫ 2.4k/3.0k [18:53<05:01, 2it/s, Loss: 0.048, PDE_losses: 0.038, BC_losses: 0.001]\n",
      "79.1%┣┫ 2.4k/3.0k [18:55<05:00, 2it/s, Loss: 0.048, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "79.2%┣┫ 2.4k/3.0k [18:56<04:58, 2it/s, Loss: 0.047, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "79.3%┣┫ 2.4k/3.0k [18:57<04:57, 2it/s, Loss: 0.048, PDE_losses: 0.038, BC_losses: 0.001]\n",
      "79.4%┣┫ 2.4k/3.0k [18:59<04:56, 2it/s, Loss: 0.050, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "79.5%┣┫ 2.4k/3.0k [19:00<04:54, 2it/s, Loss: 0.049, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "79.6%┣┫ 2.4k/3.0k [19:01<04:53, 2it/s, Loss: 0.049, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "79.7%┣┫ 2.4k/3.0k [19:02<04:51, 2it/s, Loss: 0.048, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "79.8%┣┫ 2.4k/3.0k [19:04<04:50, 2it/s, Loss: 0.048, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "79.9%┣┫ 2.4k/3.0k [19:05<04:48, 2it/s, Loss: 0.048, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "80.0%┣┫ 2.4k/3.0k [19:06<04:47, 2it/s, Loss: 0.049, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "80.1%┣┫ 2.4k/3.0k [19:07<04:45, 2it/s, Loss: 0.048, PDE_losses: 0.037, BC_losses: 0.001]\n",
      "80.2%┣┫ 2.4k/3.0k [19:09<04:44, 2it/s, Loss: 0.050, PDE_losses: 0.042, BC_losses: 0.001]\n",
      "80.3%┣┫ 2.4k/3.0k [19:10<04:42, 2it/s, Loss: 0.047, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "80.4%┣┫ 2.4k/3.0k [19:11<04:41, 2it/s, Loss: 0.053, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "80.4%┣┫ 2.4k/3.0k [19:13<04:40, 2it/s, Loss: 0.045, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "80.5%┣┫ 2.4k/3.0k [19:14<04:39, 2it/s, Loss: 0.048, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "80.6%┣┫ 2.4k/3.0k [19:15<04:38, 2it/s, Loss: 0.049, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "80.7%┣┫ 2.4k/3.0k [19:16<04:36, 2it/s, Loss: 0.050, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "80.8%┣┫ 2.4k/3.0k [19:18<04:35, 2it/s, Loss: 0.047, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "80.9%┣┫ 2.4k/3.0k [19:19<04:33, 2it/s, Loss: 0.045, PDE_losses: 0.044, BC_losses: 0.001]\n",
      "81.0%┣┫ 2.4k/3.0k [19:20<04:32, 2it/s, Loss: 0.046, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "81.1%┣┫ 2.4k/3.0k [19:21<04:30, 2it/s, Loss: 0.049, PDE_losses: 0.034, BC_losses: 0.001]\n",
      "81.2%┣┫ 2.4k/3.0k [19:22<04:29, 2it/s, Loss: 0.050, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "81.3%┣┫ 2.4k/3.0k [19:23<04:27, 2it/s, Loss: 0.045, PDE_losses: 0.038, BC_losses: 0.001]\n",
      "81.4%┣┫ 2.4k/3.0k [19:25<04:26, 2it/s, Loss: 0.049, PDE_losses: 0.043, BC_losses: 0.001]\n",
      "81.5%┣┫ 2.4k/3.0k [19:26<04:24, 2it/s, Loss: 0.049, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "81.6%┣┫ 2.4k/3.0k [19:27<04:23, 2it/s, Loss: 0.048, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "81.7%┣┫ 2.5k/3.0k [19:28<04:21, 2it/s, Loss: 0.042, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "81.8%┣┫ 2.5k/3.0k [19:30<04:20, 2it/s, Loss: 0.051, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "81.9%┣┫ 2.5k/3.0k [19:31<04:18, 2it/s, Loss: 0.048, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "82.0%┣┫ 2.5k/3.0k [19:32<04:17, 2it/s, Loss: 0.051, PDE_losses: 0.037, BC_losses: 0.001]\n",
      "82.1%┣┫ 2.5k/3.0k [19:33<04:15, 2it/s, Loss: 0.047, PDE_losses: 0.037, BC_losses: 0.001]\n",
      "82.2%┣┫ 2.5k/3.0k [19:35<04:14, 2it/s, Loss: 0.047, PDE_losses: 0.038, BC_losses: 0.001]\n",
      "82.3%┣┫ 2.5k/3.0k [19:36<04:12, 2it/s, Loss: 0.047, PDE_losses: 0.037, BC_losses: 0.001]\n",
      "82.4%┣┫ 2.5k/3.0k [19:37<04:11, 2it/s, Loss: 0.047, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "82.5%┣┫ 2.5k/3.0k [19:38<04:09, 2it/s, Loss: 0.045, PDE_losses: 0.035, BC_losses: 0.001]\n",
      "82.6%┣┫ 2.5k/3.0k [19:39<04:08, 2it/s, Loss: 0.046, PDE_losses: 0.037, BC_losses: 0.001]\n",
      "82.7%┣┫ 2.5k/3.0k [19:41<04:06, 2it/s, Loss: 0.047, PDE_losses: 0.041, BC_losses: 0.001]\n",
      "82.8%┣┫ 2.5k/3.0k [19:42<04:05, 2it/s, Loss: 0.051, PDE_losses: 0.038, BC_losses: 0.001]\n",
      "82.9%┣┫ 2.5k/3.0k [19:43<04:04, 2it/s, Loss: 0.047, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "83.0%┣┫ 2.5k/3.0k [19:44<04:02, 2it/s, Loss: 0.048, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "83.1%┣┫ 2.5k/3.0k [19:45<04:01, 2it/s, Loss: 0.043, PDE_losses: 0.036, BC_losses: 0.001]\n",
      "83.2%┣┫ 2.5k/3.0k [19:47<03:59, 2it/s, Loss: 0.042, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "83.3%┣┫ 2.5k/3.0k [19:48<03:58, 2it/s, Loss: 0.045, PDE_losses: 0.037, BC_losses: 0.001]\n",
      "83.4%┣┫ 2.5k/3.0k [19:49<03:56, 2it/s, Loss: 0.043, PDE_losses: 0.038, BC_losses: 0.001]\n",
      "83.5%┣┫ 2.5k/3.0k [19:50<03:55, 2it/s, Loss: 0.048, PDE_losses: 0.039, BC_losses: 0.001]\n",
      "83.6%┣┫ 2.5k/3.0k [19:52<03:53, 2it/s, Loss: 0.047, PDE_losses: 0.037, BC_losses: 0.000]\n",
      "83.7%┣┫ 2.5k/3.0k [19:53<03:52, 2it/s, Loss: 0.044, PDE_losses: 0.040, BC_losses: 0.001]\n",
      "83.8%┣┫ 2.5k/3.0k [19:54<03:51, 2it/s, Loss: 0.043, PDE_losses: 0.037, BC_losses: 0.000]\n",
      "83.9%┣┫ 2.5k/3.0k [19:55<03:49, 2it/s, Loss: 0.042, PDE_losses: 0.042, BC_losses: 0.000]\n",
      "84.0%┣┫ 2.5k/3.0k [19:56<03:48, 2it/s, Loss: 0.043, PDE_losses: 0.038, BC_losses: 0.000]\n",
      "84.1%┣┫ 2.5k/3.0k [19:57<03:46, 2it/s, Loss: 0.045, PDE_losses: 0.037, BC_losses: 0.000]\n",
      "84.2%┣┫ 2.5k/3.0k [19:59<03:45, 2it/s, Loss: 0.043, PDE_losses: 0.038, BC_losses: 0.000]\n",
      "84.3%┣┫ 2.5k/3.0k [20:00<03:44, 2it/s, Loss: 0.044, PDE_losses: 0.035, BC_losses: 0.000]\n",
      "84.4%┣┫ 2.5k/3.0k [20:01<03:42, 2it/s, Loss: 0.045, PDE_losses: 0.036, BC_losses: 0.000]\n",
      "84.5%┣┫ 2.5k/3.0k [20:03<03:41, 2it/s, Loss: 0.042, PDE_losses: 0.040, BC_losses: 0.000]\n",
      "84.6%┣┫ 2.5k/3.0k [20:04<03:39, 2it/s, Loss: 0.046, PDE_losses: 0.036, BC_losses: 0.000]\n",
      "84.7%┣┫ 2.5k/3.0k [20:05<03:38, 2it/s, Loss: 0.048, PDE_losses: 0.037, BC_losses: 0.000]\n",
      "84.8%┣┫ 2.5k/3.0k [20:06<03:36, 2it/s, Loss: 0.044, PDE_losses: 0.036, BC_losses: 0.000]\n",
      "84.9%┣┫ 2.5k/3.0k [20:07<03:35, 2it/s, Loss: 0.042, PDE_losses: 0.039, BC_losses: 0.000]\n",
      "85.0%┣┫ 2.5k/3.0k [20:09<03:33, 2it/s, Loss: 0.045, PDE_losses: 0.037, BC_losses: 0.000]\n",
      "85.1%┣┫ 2.6k/3.0k [20:10<03:32, 2it/s, Loss: 0.045, PDE_losses: 0.036, BC_losses: 0.000]\n",
      "85.2%┣┫ 2.6k/3.0k [20:11<03:31, 2it/s, Loss: 0.046, PDE_losses: 0.036, BC_losses: 0.000]\n",
      "85.3%┣┫ 2.6k/3.0k [20:13<03:29, 2it/s, Loss: 0.046, PDE_losses: 0.033, BC_losses: 0.000]\n",
      "85.4%┣┫ 2.6k/3.0k [20:14<03:28, 2it/s, Loss: 0.042, PDE_losses: 0.039, BC_losses: 0.000]\n",
      "85.5%┣┫ 2.6k/3.0k [20:15<03:26, 2it/s, Loss: 0.044, PDE_losses: 0.035, BC_losses: 0.000]\n",
      "85.6%┣┫ 2.6k/3.0k [20:16<03:25, 2it/s, Loss: 0.043, PDE_losses: 0.034, BC_losses: 0.000]\n",
      "85.7%┣┫ 2.6k/3.0k [20:18<03:23, 2it/s, Loss: 0.047, PDE_losses: 0.037, BC_losses: 0.000]\n",
      "85.8%┣┫ 2.6k/3.0k [20:19<03:22, 2it/s, Loss: 0.041, PDE_losses: 0.033, BC_losses: 0.000]\n",
      "85.9%┣┫ 2.6k/3.0k [20:20<03:20, 2it/s, Loss: 0.041, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "86.0%┣┫ 2.6k/3.0k [20:21<03:19, 2it/s, Loss: 0.039, PDE_losses: 0.037, BC_losses: 0.000]\n",
      "86.1%┣┫ 2.6k/3.0k [20:23<03:17, 2it/s, Loss: 0.039, PDE_losses: 0.040, BC_losses: 0.000]\n",
      "86.2%┣┫ 2.6k/3.0k [20:24<03:16, 2it/s, Loss: 0.043, PDE_losses: 0.034, BC_losses: 0.000]\n",
      "86.3%┣┫ 2.6k/3.0k [20:25<03:15, 2it/s, Loss: 0.043, PDE_losses: 0.039, BC_losses: 0.000]\n",
      "86.4%┣┫ 2.6k/3.0k [20:27<03:13, 2it/s, Loss: 0.043, PDE_losses: 0.037, BC_losses: 0.000]\n",
      "86.5%┣┫ 2.6k/3.0k [20:28<03:12, 2it/s, Loss: 0.046, PDE_losses: 0.037, BC_losses: 0.000]\n",
      "86.6%┣┫ 2.6k/3.0k [20:29<03:10, 2it/s, Loss: 0.046, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "86.7%┣┫ 2.6k/3.0k [20:30<03:09, 2it/s, Loss: 0.040, PDE_losses: 0.035, BC_losses: 0.000]\n",
      "86.8%┣┫ 2.6k/3.0k [20:32<03:07, 2it/s, Loss: 0.040, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "86.9%┣┫ 2.6k/3.0k [20:33<03:06, 2it/s, Loss: 0.044, PDE_losses: 0.034, BC_losses: 0.000]\n",
      "87.0%┣┫ 2.6k/3.0k [20:34<03:04, 2it/s, Loss: 0.040, PDE_losses: 0.035, BC_losses: 0.000]\n",
      "87.1%┣┫ 2.6k/3.0k [20:35<03:03, 2it/s, Loss: 0.041, PDE_losses: 0.036, BC_losses: 0.000]\n",
      "87.2%┣┫ 2.6k/3.0k [20:36<03:02, 2it/s, Loss: 0.043, PDE_losses: 0.038, BC_losses: 0.000]\n",
      "87.3%┣┫ 2.6k/3.0k [20:38<03:00, 2it/s, Loss: 0.044, PDE_losses: 0.035, BC_losses: 0.000]\n",
      "87.4%┣┫ 2.6k/3.0k [20:39<02:59, 2it/s, Loss: 0.043, PDE_losses: 0.033, BC_losses: 0.000]\n",
      "87.5%┣┫ 2.6k/3.0k [20:40<02:57, 2it/s, Loss: 0.038, PDE_losses: 0.034, BC_losses: 0.000]\n",
      "87.6%┣┫ 2.6k/3.0k [20:41<02:56, 2it/s, Loss: 0.039, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "87.7%┣┫ 2.6k/3.0k [20:42<02:54, 2it/s, Loss: 0.040, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "87.8%┣┫ 2.6k/3.0k [20:43<02:53, 2it/s, Loss: 0.047, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "87.9%┣┫ 2.6k/3.0k [20:45<02:51, 2it/s, Loss: 0.041, PDE_losses: 0.034, BC_losses: 0.000]\n",
      "88.0%┣┫ 2.6k/3.0k [20:46<02:50, 2it/s, Loss: 0.046, PDE_losses: 0.036, BC_losses: 0.000]\n",
      "88.1%┣┫ 2.6k/3.0k [20:47<02:49, 2it/s, Loss: 0.041, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "88.2%┣┫ 2.6k/3.0k [20:49<02:47, 2it/s, Loss: 0.042, PDE_losses: 0.033, BC_losses: 0.000]\n",
      "88.3%┣┫ 2.6k/3.0k [20:50<02:46, 2it/s, Loss: 0.042, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "88.4%┣┫ 2.7k/3.0k [20:51<02:44, 2it/s, Loss: 0.040, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "88.5%┣┫ 2.7k/3.0k [20:52<02:43, 2it/s, Loss: 0.040, PDE_losses: 0.034, BC_losses: 0.000]\n",
      "88.6%┣┫ 2.7k/3.0k [20:54<02:41, 2it/s, Loss: 0.038, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "88.7%┣┫ 2.7k/3.0k [20:55<02:40, 2it/s, Loss: 0.041, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "88.8%┣┫ 2.7k/3.0k [20:56<02:39, 2it/s, Loss: 0.041, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "88.9%┣┫ 2.7k/3.0k [20:57<02:37, 2it/s, Loss: 0.038, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "89.0%┣┫ 2.7k/3.0k [20:59<02:36, 2it/s, Loss: 0.043, PDE_losses: 0.034, BC_losses: 0.000]\n",
      "89.1%┣┫ 2.7k/3.0k [21:00<02:34, 2it/s, Loss: 0.042, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "89.2%┣┫ 2.7k/3.0k [21:01<02:33, 2it/s, Loss: 0.037, PDE_losses: 0.030, BC_losses: 0.000]\n",
      "89.3%┣┫ 2.7k/3.0k [21:02<02:31, 2it/s, Loss: 0.041, PDE_losses: 0.033, BC_losses: 0.000]\n",
      "89.4%┣┫ 2.7k/3.0k [21:03<02:30, 2it/s, Loss: 0.039, PDE_losses: 0.030, BC_losses: 0.000]\n",
      "89.5%┣┫ 2.7k/3.0k [21:05<02:28, 2it/s, Loss: 0.041, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "89.6%┣┫ 2.7k/3.0k [21:06<02:27, 2it/s, Loss: 0.036, PDE_losses: 0.035, BC_losses: 0.000]\n",
      "89.7%┣┫ 2.7k/3.0k [21:07<02:26, 2it/s, Loss: 0.040, PDE_losses: 0.030, BC_losses: 0.000]\n",
      "89.8%┣┫ 2.7k/3.0k [21:08<02:24, 2it/s, Loss: 0.035, PDE_losses: 0.036, BC_losses: 0.000]\n",
      "89.9%┣┫ 2.7k/3.0k [21:10<02:23, 2it/s, Loss: 0.040, PDE_losses: 0.033, BC_losses: 0.000]\n",
      "90.0%┣┫ 2.7k/3.0k [21:11<02:21, 2it/s, Loss: 0.042, PDE_losses: 0.033, BC_losses: 0.000]\n",
      "90.1%┣┫ 2.7k/3.0k [21:12<02:20, 2it/s, Loss: 0.037, PDE_losses: 0.034, BC_losses: 0.000]\n",
      "90.2%┣┫ 2.7k/3.0k [21:13<02:18, 2it/s, Loss: 0.043, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "90.3%┣┫ 2.7k/3.0k [21:15<02:17, 2it/s, Loss: 0.040, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "90.4%┣┫ 2.7k/3.0k [21:16<02:16, 2it/s, Loss: 0.038, PDE_losses: 0.035, BC_losses: 0.000]\n",
      "90.5%┣┫ 2.7k/3.0k [21:17<02:14, 2it/s, Loss: 0.039, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "90.6%┣┫ 2.7k/3.0k [21:18<02:13, 2it/s, Loss: 0.038, PDE_losses: 0.035, BC_losses: 0.000]\n",
      "90.7%┣┫ 2.7k/3.0k [21:20<02:11, 2it/s, Loss: 0.041, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "90.8%┣┫ 2.7k/3.0k [21:21<02:10, 2it/s, Loss: 0.041, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "90.9%┣┫ 2.7k/3.0k [21:22<02:08, 2it/s, Loss: 0.041, PDE_losses: 0.034, BC_losses: 0.000]\n",
      "91.0%┣┫ 2.7k/3.0k [21:23<02:07, 2it/s, Loss: 0.036, PDE_losses: 0.033, BC_losses: 0.000]\n",
      "91.1%┣┫ 2.7k/3.0k [21:24<02:06, 2it/s, Loss: 0.039, PDE_losses: 0.030, BC_losses: 0.000]\n",
      "91.2%┣┫ 2.7k/3.0k [21:26<02:04, 2it/s, Loss: 0.039, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "91.3%┣┫ 2.7k/3.0k [21:27<02:03, 2it/s, Loss: 0.038, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "91.4%┣┫ 2.7k/3.0k [21:28<02:01, 2it/s, Loss: 0.039, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "91.5%┣┫ 2.7k/3.0k [21:30<02:00, 2it/s, Loss: 0.038, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "91.6%┣┫ 2.7k/3.0k [21:31<01:58, 2it/s, Loss: 0.036, PDE_losses: 0.030, BC_losses: 0.000]\n",
      "91.7%┣┫ 2.8k/3.0k [21:32<01:57, 2it/s, Loss: 0.036, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "91.8%┣┫ 2.8k/3.0k [21:34<01:56, 2it/s, Loss: 0.038, PDE_losses: 0.035, BC_losses: 0.000]\n",
      "91.9%┣┫ 2.8k/3.0k [21:35<01:54, 2it/s, Loss: 0.038, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "92.0%┣┫ 2.8k/3.0k [21:36<01:53, 2it/s, Loss: 0.042, PDE_losses: 0.036, BC_losses: 0.000]\n",
      "92.1%┣┫ 2.8k/3.0k [21:37<01:51, 2it/s, Loss: 0.038, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "92.2%┣┫ 2.8k/3.0k [21:38<01:50, 2it/s, Loss: 0.037, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "92.3%┣┫ 2.8k/3.0k [21:40<01:48, 2it/s, Loss: 0.038, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "92.4%┣┫ 2.8k/3.0k [21:41<01:47, 2it/s, Loss: 0.037, PDE_losses: 0.030, BC_losses: 0.000]\n",
      "92.5%┣┫ 2.8k/3.0k [21:42<01:46, 2it/s, Loss: 0.041, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "92.6%┣┫ 2.8k/3.0k [21:43<01:44, 2it/s, Loss: 0.041, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "92.7%┣┫ 2.8k/3.0k [21:45<01:43, 2it/s, Loss: 0.039, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "92.8%┣┫ 2.8k/3.0k [21:46<01:41, 2it/s, Loss: 0.038, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "92.9%┣┫ 2.8k/3.0k [21:47<01:40, 2it/s, Loss: 0.041, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "93.0%┣┫ 2.8k/3.0k [21:49<01:39, 2it/s, Loss: 0.034, PDE_losses: 0.027, BC_losses: 0.000]\n",
      "93.1%┣┫ 2.8k/3.0k [21:50<01:37, 2it/s, Loss: 0.035, PDE_losses: 0.033, BC_losses: 0.000]\n",
      "93.2%┣┫ 2.8k/3.0k [21:51<01:36, 2it/s, Loss: 0.037, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "93.3%┣┫ 2.8k/3.0k [21:52<01:34, 2it/s, Loss: 0.040, PDE_losses: 0.030, BC_losses: 0.000]\n",
      "93.4%┣┫ 2.8k/3.0k [21:54<01:33, 2it/s, Loss: 0.036, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "93.5%┣┫ 2.8k/3.0k [21:55<01:31, 2it/s, Loss: 0.039, PDE_losses: 0.026, BC_losses: 0.000]\n",
      "93.6%┣┫ 2.8k/3.0k [21:57<01:31, 2it/s, Loss: 0.038, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "93.7%┣┫ 2.8k/3.0k [21:58<01:29, 2it/s, Loss: 0.035, PDE_losses: 0.030, BC_losses: 0.000]\n",
      "93.8%┣┫ 2.8k/3.0k [21:59<01:28, 2it/s, Loss: 0.035, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "93.9%┣┫ 2.8k/3.0k [22:01<01:26, 2it/s, Loss: 0.032, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "94.0%┣┫ 2.8k/3.0k [22:02<01:25, 2it/s, Loss: 0.035, PDE_losses: 0.027, BC_losses: 0.000]\n",
      "94.1%┣┫ 2.8k/3.0k [22:03<01:23, 2it/s, Loss: 0.038, PDE_losses: 0.030, BC_losses: 0.000]\n",
      "94.2%┣┫ 2.8k/3.0k [22:05<01:22, 2it/s, Loss: 0.035, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "94.3%┣┫ 2.8k/3.0k [22:06<01:21, 2it/s, Loss: 0.036, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "94.4%┣┫ 2.8k/3.0k [22:07<01:19, 2it/s, Loss: 0.036, PDE_losses: 0.026, BC_losses: 0.000]\n",
      "94.5%┣┫ 2.8k/3.0k [22:09<01:18, 2it/s, Loss: 0.033, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "94.6%┣┫ 2.8k/3.0k [22:10<01:16, 2it/s, Loss: 0.034, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "94.7%┣┫ 2.8k/3.0k [22:11<01:15, 2it/s, Loss: 0.037, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "94.8%┣┫ 2.8k/3.0k [22:12<01:14, 2it/s, Loss: 0.039, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "94.9%┣┫ 2.8k/3.0k [22:14<01:12, 2it/s, Loss: 0.037, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "95.0%┣┫ 2.8k/3.0k [22:15<01:11, 2it/s, Loss: 0.034, PDE_losses: 0.026, BC_losses: 0.000]\n",
      "95.1%┣┫ 2.9k/3.0k [22:16<01:09, 2it/s, Loss: 0.034, PDE_losses: 0.023, BC_losses: 0.000]\n",
      "95.2%┣┫ 2.9k/3.0k [22:18<01:08, 2it/s, Loss: 0.037, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "95.3%┣┫ 2.9k/3.0k [22:19<01:07, 2it/s, Loss: 0.036, PDE_losses: 0.027, BC_losses: 0.000]\n",
      "95.4%┣┫ 2.9k/3.0k [22:20<01:05, 2it/s, Loss: 0.035, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "95.5%┣┫ 2.9k/3.0k [22:21<01:04, 2it/s, Loss: 0.034, PDE_losses: 0.025, BC_losses: 0.000]\n",
      "95.6%┣┫ 2.9k/3.0k [22:23<01:02, 2it/s, Loss: 0.033, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "95.7%┣┫ 2.9k/3.0k [22:24<01:01, 2it/s, Loss: 0.034, PDE_losses: 0.030, BC_losses: 0.000]\n",
      "95.8%┣┫ 2.9k/3.0k [22:26<01:00, 2it/s, Loss: 0.033, PDE_losses: 0.026, BC_losses: 0.000]\n",
      "95.9%┣┫ 2.9k/3.0k [22:27<00:58, 2it/s, Loss: 0.035, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "96.0%┣┫ 2.9k/3.0k [22:28<00:57, 2it/s, Loss: 0.038, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "96.1%┣┫ 2.9k/3.0k [22:29<00:55, 2it/s, Loss: 0.036, PDE_losses: 0.026, BC_losses: 0.000]\n",
      "96.2%┣┫ 2.9k/3.0k [22:31<00:54, 2it/s, Loss: 0.036, PDE_losses: 0.025, BC_losses: 0.000]\n",
      "96.3%┣┫ 2.9k/3.0k [22:32<00:52, 2it/s, Loss: 0.033, PDE_losses: 0.026, BC_losses: 0.000]\n",
      "96.4%┣┫ 2.9k/3.0k [22:33<00:51, 2it/s, Loss: 0.035, PDE_losses: 0.026, BC_losses: 0.000]\n",
      "96.5%┣┫ 2.9k/3.0k [22:35<00:50, 2it/s, Loss: 0.034, PDE_losses: 0.022, BC_losses: 0.000]\n",
      "96.6%┣┫ 2.9k/3.0k [22:36<00:48, 2it/s, Loss: 0.030, PDE_losses: 0.022, BC_losses: 0.000]\n",
      "96.6%┣┫ 2.9k/3.0k [22:37<00:47, 2it/s, Loss: 0.031, PDE_losses: 0.029, BC_losses: 0.000]\n",
      "96.7%┣┫ 2.9k/3.0k [22:38<00:46, 2it/s, Loss: 0.030, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "96.8%┣┫ 2.9k/3.0k [22:40<00:44, 2it/s, Loss: 0.032, PDE_losses: 0.023, BC_losses: 0.000]\n",
      "96.9%┣┫ 2.9k/3.0k [22:41<00:43, 2it/s, Loss: 0.033, PDE_losses: 0.027, BC_losses: 0.000]\n",
      "97.0%┣┫ 2.9k/3.0k [22:42<00:42, 2it/s, Loss: 0.030, PDE_losses: 0.032, BC_losses: 0.000]\n",
      "97.1%┣┫ 2.9k/3.0k [22:43<00:40, 2it/s, Loss: 0.033, PDE_losses: 0.023, BC_losses: 0.000]\n",
      "97.2%┣┫ 2.9k/3.0k [22:44<00:39, 2it/s, Loss: 0.031, PDE_losses: 0.022, BC_losses: 0.000]\n",
      "97.3%┣┫ 2.9k/3.0k [22:46<00:37, 2it/s, Loss: 0.030, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "97.4%┣┫ 2.9k/3.0k [22:47<00:36, 2it/s, Loss: 0.032, PDE_losses: 0.024, BC_losses: 0.000]\n",
      "97.5%┣┫ 2.9k/3.0k [22:48<00:35, 2it/s, Loss: 0.033, PDE_losses: 0.023, BC_losses: 0.000]\n",
      "97.6%┣┫ 2.9k/3.0k [22:50<00:33, 2it/s, Loss: 0.033, PDE_losses: 0.031, BC_losses: 0.000]\n",
      "97.7%┣┫ 2.9k/3.0k [22:51<00:32, 2it/s, Loss: 0.033, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "97.8%┣┫ 2.9k/3.0k [22:52<00:30, 2it/s, Loss: 0.029, PDE_losses: 0.026, BC_losses: 0.000]\n",
      "97.9%┣┫ 2.9k/3.0k [22:53<00:29, 2it/s, Loss: 0.031, PDE_losses: 0.027, BC_losses: 0.000]\n",
      "98.0%┣┫ 2.9k/3.0k [22:55<00:28, 2it/s, Loss: 0.030, PDE_losses: 0.024, BC_losses: 0.000]\n",
      "98.1%┣┫ 2.9k/3.0k [22:56<00:26, 2it/s, Loss: 0.032, PDE_losses: 0.026, BC_losses: 0.000]\n",
      "98.2%┣┫ 2.9k/3.0k [22:57<00:25, 2it/s, Loss: 0.033, PDE_losses: 0.027, BC_losses: 0.000]\n",
      "98.3%┣┫ 3.0k/3.0k [22:59<00:23, 2it/s, Loss: 0.029, PDE_losses: 0.025, BC_losses: 0.000]\n",
      "98.4%┣┫ 3.0k/3.0k [23:00<00:22, 2it/s, Loss: 0.033, PDE_losses: 0.019, BC_losses: 0.000]\n",
      "98.5%┣┫ 3.0k/3.0k [23:01<00:21, 2it/s, Loss: 0.032, PDE_losses: 0.026, BC_losses: 0.000]\n",
      "98.6%┣┫ 3.0k/3.0k [23:03<00:19, 2it/s, Loss: 0.033, PDE_losses: 0.026, BC_losses: 0.000]\n",
      "98.7%┣┫ 3.0k/3.0k [23:04<00:18, 2it/s, Loss: 0.032, PDE_losses: 0.023, BC_losses: 0.000]\n",
      "98.8%┣┫ 3.0k/3.0k [23:05<00:16, 2it/s, Loss: 0.031, PDE_losses: 0.022, BC_losses: 0.000]\n",
      "98.9%┣┫ 3.0k/3.0k [23:06<00:15, 2it/s, Loss: 0.029, PDE_losses: 0.024, BC_losses: 0.000]\n",
      "99.0%┣┫ 3.0k/3.0k [23:08<00:14, 2it/s, Loss: 0.031, PDE_losses: 0.027, BC_losses: 0.000]\n",
      "99.1%┣┫ 3.0k/3.0k [23:09<00:12, 2it/s, Loss: 0.029, PDE_losses: 0.028, BC_losses: 0.000]\n",
      "99.2%┣┫ 3.0k/3.0k [23:10<00:11, 2it/s, Loss: 0.028, PDE_losses: 0.024, BC_losses: 0.000]\n",
      "99.3%┣┫ 3.0k/3.0k [23:11<00:09, 2it/s, Loss: 0.032, PDE_losses: 0.026, BC_losses: 0.000]\n",
      "99.4%┣┫ 3.0k/3.0k [23:13<00:08, 2it/s, Loss: 0.028, PDE_losses: 0.023, BC_losses: 0.000]\n",
      "99.5%┣┫ 3.0k/3.0k [23:14<00:07, 2it/s, Loss: 0.034, PDE_losses: 0.021, BC_losses: 0.000]\n",
      "99.6%┣┫ 3.0k/3.0k [23:15<00:05, 2it/s, Loss: 0.031, PDE_losses: 0.024, BC_losses: 0.000]\n",
      "99.7%┣┫ 3.0k/3.0k [23:17<00:04, 2it/s, Loss: 0.030, PDE_losses: 0.023, BC_losses: 0.000]\n",
      "99.8%┣┫ 3.0k/3.0k [23:18<00:02, 2it/s, Loss: 0.029, PDE_losses: 0.024, BC_losses: 0.000]\n",
      "99.9%┣┫ 3.0k/3.0k [23:19<00:01, 2it/s, Loss: 0.030, PDE_losses: 0.019, BC_losses: 0.000]\n",
      "100.0%┣┫ 3.0k/3.0k [23:20<00:00, 2it/s, Loss: 0.029, PDE_losses: 0.024, BC_losses: 0.000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralPDE.Phi{StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}(StatefulLuxLayer{Static.True, Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}(Chain{@NamedTuple{layer_1::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_3::Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_4::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(4 => 32, σ), layer_2 = Dense(32 => 32, σ), layer_3 = Dense(32 => 32, σ), layer_4 = Dense(32 => 8)), nothing), nothing, (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()), nothing, static(true)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxiters = 3000\n",
    "callback = create_callback(maxiters)\n",
    "# Решение\n",
    "res = solve(prob, opt; maxiters = maxiters, callback)\n",
    "phi = discretization.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2005704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0mComponentVector{Float32, CuArray{Float32, 1, CUDA.DeviceMemory}, Tuple{Axis{(layer_1 = ViewAxis(1:160, Axis(weight = ViewAxis(1:128, ShapedAxis((32, 4))), bias = ViewAxis(129:160, Shaped1DAxis((32,))))), layer_2 = ViewAxis(161:1216, Axis(weight = ViewAxis(1:1024, ShapedAxis((32, 32))), bias = ViewAxis(1025:1056, Shaped1DAxis((32,))))), layer_3 = ViewAxis(1217:2272, Axis(weight = ViewAxis(1:1024, ShapedAxis((32, 32))), bias = ViewAxis(1025:1056, Shaped1DAxis((32,))))), layer_4 = ViewAxis(2273:2536, Axis(weight = ViewAxis(1:256, ShapedAxis((8, 32))), bias = ViewAxis(257:264, Shaped1DAxis((8,))))))}}}(layer_1 = (weight = Float32[0.8065516 0.5839059 -0.3802054 -0.5337242; -0.115212545 0.219565 0.4828236 -0.5400207; … ; -0.02438163 -0.035658043 -0.31350365 1.0160261; -0.47713172 -0.289386 -0.33993164 -0.2496976], bias = Float32[1.219842, 0.55122685, 1.4108653, 0.8409013, 0.55615115, 1.0280206, -0.74936604, -1.1786482, 0.31820184, 0.12179776  …  0.56855696, -1.0489694, -1.1314857, -1.3110793, -0.7373667, -0.13086152, 0.03215375, -0.3372667, 0.26320854, 0.22187065]), layer_2 = (weight = Float32[0.29713 0.01549216 … -0.24398404 -0.041434135; -0.0060513923 0.22857766 … 0.12171123 -0.25843328; … ; 0.110803105 -0.2437925 … 0.23836802 0.051222026; 0.18508072 -0.25232738 … -0.019773012 -0.023032153], bias = Float32[0.0144859785, 0.00995838, 0.04503613, -0.036814664, -0.16009991, 0.08312542, 0.06841652, 0.08816993, -0.08861145, -0.03207097  …  -0.011135595, -0.15431266, 0.09196291, 0.023439942, 0.1820921, -0.1328843, -0.12266624, -0.30909047, 0.018678837, 0.13804923]), layer_3 = (weight = Float32[0.110941276 0.22004585 … 0.2542474 -0.31822655; -0.018766463 -0.13447173 … -0.26785856 -0.12715776; … ; 0.085866414 -0.25640103 … -0.0736438 0.174389; -0.2154934 0.1297596 … -0.17856066 0.17752276], bias = Float32[-0.07791661, -0.12428381, 0.087455854, -0.07482763, -0.0009010673, -0.14076898, -0.024265382, 0.040038794, -0.08897876, -0.13838468  …  -0.011776123, -0.10774637, -0.052805, 0.04895837, -0.17102683, 0.16147204, 0.13638808, -0.03076956, 0.007777443, 0.17545652]), layer_4 = (weight = Float32[-0.379791 -0.24679582 … 0.037762802 0.02683969; -0.2675759 0.16962333 … -0.2216275 0.022105513; … ; 0.17704195 -0.05174897 … -0.090602234 0.27627987; 0.12365203 -0.11426543 … -0.044839438 0.12735839], bias = Float32[-0.07344405, 0.08748816, -0.015658913, 0.13534382, -0.029775532, -0.09064311, 0.11765017, -0.06926782]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b27568b",
   "metadata": {},
   "source": [
    "### График скалярного потенциала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e30b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/sasha/inverse-npde/figures/plot_phi.png\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "\n",
    "phi = discretization.phi\n",
    "xs, ys, zs, ts = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\n",
    "\n",
    "minimizers_ = res.u|> cpud\n",
    "z_selected = 0.0\n",
    "t_selected = 0.0\n",
    "clip = 10\n",
    "u_real = [clamp(analytic_sol_func(0, xs, ys, z_selected), -clip, clip) for xs in xs for ys in ys]\n",
    "u_predict = [(phi([x, y, z_selected, t_selected], minimizers_))[1] for x in xs for y in ys ]\n",
    "diff_u = [abs.(u_real .- u_predict)]\n",
    "\n",
    "ps = []\n",
    "\n",
    "p1 = plot(xs, ys, u_real, linetype = :contourf, title = \"phi, analytic\")\n",
    "p2 = plot(xs, ys, u_predict, linetype = :contourf, title = \"phi, predict\")\n",
    "p3 = plot(xs, ys, diff_u, linetype = :contourf, title = \"phi, error\")\n",
    "push!(ps, plot(p1, p2, p3))\n",
    "\n",
    "\n",
    "# Сохранение графиков\n",
    "savefig(ps[1], \"../../figures/plot_phi.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f3009",
   "metadata": {},
   "source": [
    "### График плотности заряда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d9179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#32 (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Плотность заряда\n",
    "sigma = 1\n",
    "charge = Distributions.MvNormal([0.; 0.; 0.], [1. 0.0 0.0; 0.0 1. 0.0; 0.0 0.00 1.] * sigma)\n",
    "rho = (t, x, y, z) -> Distributions.pdf(charge, [x; y; z]) * ((t + 1)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42ec3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/sasha/inverse-npde/figures/plot_rho.png\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "\n",
    "phi = discretization.phi\n",
    "xs, ys, zs, ts = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\n",
    "\n",
    "minimizers_ = res.u|> cpud\n",
    "z_selected = 0.0\n",
    "t_selected = 0.0\n",
    "clip = 10\n",
    "u_real = [clamp(rho(0, xs, ys, z_selected), -clip, clip) for xs in xs for ys in ys]\n",
    "u_predict = [(phi([x, y, z_selected, t_selected], minimizers_))[5] for x in xs for y in ys ]\n",
    "diff_u = [abs.(u_real .- u_predict)]\n",
    "\n",
    "ps = []\n",
    "\n",
    "p1 = plot(xs, ys, u_real, linetype = :contourf, title = \"rho, analytic\")\n",
    "p2 = plot(xs, ys, u_predict, linetype = :contourf, title = \"rho, predict\")\n",
    "p3 = plot(xs, ys, diff_u, linetype = :contourf, title = \"rho, error\")\n",
    "push!(ps, plot(p1, p2, p3))\n",
    "\n",
    "\n",
    "# Сохранение графиков\n",
    "savefig(ps[1], \"../../figures/plot_rho.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a74093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
